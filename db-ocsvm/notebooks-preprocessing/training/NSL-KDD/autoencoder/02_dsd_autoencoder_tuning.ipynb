{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSparseDenoisingAutoencoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        hidden_dims=[\n",
    "            64,\n",
    "            32,\n",
    "            16,\n",
    "        ],  # Dimensions of encoder layers (decoder will be symmetric)\n",
    "        activation=\"relu\",\n",
    "        dropout_rate=0.2,\n",
    "        noise_factor=0.2,\n",
    "        sparsity_weight=1e-3,\n",
    "        sparsity_target=0.05,\n",
    "    ):\n",
    "        super(DeepSparseDenoisingAutoencoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.bottleneck_dim = hidden_dims[-1]\n",
    "        self.noise_factor = noise_factor\n",
    "        self.sparsity_weight = sparsity_weight\n",
    "        self.sparsity_target = sparsity_target\n",
    "\n",
    "        # Set activation function\n",
    "        if activation == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "        elif activation == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(f\"Activation {activation} not supported\")\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Build encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        for dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(prev_dim, dim))\n",
    "            encoder_layers.append(self.activation)\n",
    "            encoder_layers.append(self.dropout)\n",
    "            prev_dim = dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Build decoder (symmetric to encoder)\n",
    "        decoder_layers = []\n",
    "        hidden_dims_reversed = list(reversed(hidden_dims))\n",
    "\n",
    "        prev_dim = hidden_dims[-1]  # Start from bottleneck\n",
    "\n",
    "        for i, dim in enumerate(hidden_dims_reversed):\n",
    "            if i < len(hidden_dims_reversed) - 1:\n",
    "                next_dim = hidden_dims_reversed[i + 1]\n",
    "            else:\n",
    "                next_dim = input_dim\n",
    "\n",
    "            decoder_layers.append(nn.Linear(prev_dim, next_dim))\n",
    "\n",
    "            # Only add activation for all but the last layer\n",
    "            if i < len(hidden_dims_reversed) - 1:\n",
    "                decoder_layers.append(self.activation)\n",
    "                decoder_layers.append(self.dropout)\n",
    "            else:\n",
    "                # Output layer activation is sigmoid since data is normalized to [0,1]\n",
    "                decoder_layers.append(nn.Sigmoid())\n",
    "                pass\n",
    "\n",
    "            prev_dim = next_dim\n",
    "\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def add_noise(self, x):\n",
    "        # Add Gaussian noise for denoising capability\n",
    "        noise = torch.randn_like(x) * self.noise_factor\n",
    "        return x + noise\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add noise to input (only during training)\n",
    "        if self.training:\n",
    "            x_noisy = self.add_noise(x)\n",
    "        else:\n",
    "            x_noisy = x\n",
    "\n",
    "        # Encode\n",
    "        encoded = self.encoder(x_noisy)\n",
    "\n",
    "        # Decode\n",
    "        decoded = self.decoder(encoded)\n",
    "\n",
    "        return decoded, encoded\n",
    "\n",
    "    def get_bottleneck_representation(self, x):\n",
    "        \"\"\"Extract the bottleneck features for dimensionality reduction\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.eval()  # Set to evaluation mode\n",
    "            encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "    def kl_divergence_loss(self, activations):\n",
    "        \"\"\"Calculate KL divergence for sparsity constraint\"\"\"\n",
    "        # Average activation across batch\n",
    "        rho_hat = torch.mean(activations, dim=0)\n",
    "        # KL divergence between rho_hat and target sparsity level\n",
    "        kl_loss = self.sparsity_target * torch.log(\n",
    "            self.sparsity_target / (rho_hat + 1e-10)\n",
    "        ) + (1 - self.sparsity_target) * torch.log(\n",
    "            (1 - self.sparsity_target) / (1 - rho_hat + 1e-10)\n",
    "        )\n",
    "        return torch.sum(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(\n",
    "    model,\n",
    "    dataloader,\n",
    "    epochs=30,\n",
    "    learning_rate=0.0001,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    reconstruction_criterion = nn.MSELoss()\n",
    "\n",
    "    history = {\"loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for data in dataloader:\n",
    "            inputs = data[0].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs, encoded = model(inputs)\n",
    "\n",
    "            # Calculate loss\n",
    "            reconstruction_loss = reconstruction_criterion(outputs, inputs)\n",
    "            sparsity_loss = model.kl_divergence_loss(encoded)\n",
    "            total_loss = reconstruction_loss + model.sparsity_weight * sparsity_loss\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += total_loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(dataloader)\n",
    "        history[\"loss\"].append(avg_train_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5387, 122)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_REJ</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28282</th>\n",
       "      <td>-0.102571</td>\n",
       "      <td>-0.007723</td>\n",
       "      <td>-0.004728</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089487</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>1.765416</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235686</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.31289</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139983</td>\n",
       "      <td>-0.618441</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031768</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825156</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19433</th>\n",
       "      <td>-0.110250</td>\n",
       "      <td>-0.007666</td>\n",
       "      <td>-0.004919</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089487</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809267</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.31289</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139983</td>\n",
       "      <td>-0.618441</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031768</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825156</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30618</th>\n",
       "      <td>-0.110250</td>\n",
       "      <td>-0.007733</td>\n",
       "      <td>-0.004414</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089487</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235686</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.31289</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139983</td>\n",
       "      <td>-0.618441</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031768</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825156</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14731</th>\n",
       "      <td>-0.110250</td>\n",
       "      <td>-0.007674</td>\n",
       "      <td>-0.004918</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089487</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>-0.809267</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.31289</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139983</td>\n",
       "      <td>-0.618441</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031768</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825156</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26126</th>\n",
       "      <td>-0.110250</td>\n",
       "      <td>-0.007642</td>\n",
       "      <td>-0.004835</td>\n",
       "      <td>-0.014089</td>\n",
       "      <td>-0.089487</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>-0.095076</td>\n",
       "      <td>-0.027023</td>\n",
       "      <td>1.235686</td>\n",
       "      <td>-0.011664</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.31289</td>\n",
       "      <td>-0.11205</td>\n",
       "      <td>-0.028606</td>\n",
       "      <td>-0.139983</td>\n",
       "      <td>-0.618441</td>\n",
       "      <td>-0.053906</td>\n",
       "      <td>-0.031768</td>\n",
       "      <td>-0.019726</td>\n",
       "      <td>0.825156</td>\n",
       "      <td>-0.046432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       duration  src_bytes  dst_bytes      land  wrong_fragment    urgent  \\\n",
       "28282 -0.102571  -0.007723  -0.004728 -0.014089       -0.089487 -0.007736   \n",
       "19433 -0.110250  -0.007666  -0.004919 -0.014089       -0.089487 -0.007736   \n",
       "30618 -0.110250  -0.007733  -0.004414 -0.014089       -0.089487 -0.007736   \n",
       "14731 -0.110250  -0.007674  -0.004918 -0.014089       -0.089487 -0.007736   \n",
       "26126 -0.110250  -0.007642  -0.004835 -0.014089       -0.089487 -0.007736   \n",
       "\n",
       "            hot  num_failed_logins  logged_in  num_compromised  ...  flag_REJ  \\\n",
       "28282  1.765416          -0.027023   1.235686        -0.011664  ...  -0.31289   \n",
       "19433 -0.095076          -0.027023  -0.809267        -0.011664  ...  -0.31289   \n",
       "30618 -0.095076          -0.027023   1.235686        -0.011664  ...  -0.31289   \n",
       "14731 -0.095076          -0.027023  -0.809267        -0.011664  ...  -0.31289   \n",
       "26126 -0.095076          -0.027023   1.235686        -0.011664  ...  -0.31289   \n",
       "\n",
       "       flag_RSTO  flag_RSTOS0  flag_RSTR   flag_S0   flag_S1   flag_S2  \\\n",
       "28282   -0.11205    -0.028606  -0.139983 -0.618441 -0.053906 -0.031768   \n",
       "19433   -0.11205    -0.028606  -0.139983 -0.618441 -0.053906 -0.031768   \n",
       "30618   -0.11205    -0.028606  -0.139983 -0.618441 -0.053906 -0.031768   \n",
       "14731   -0.11205    -0.028606  -0.139983 -0.618441 -0.053906 -0.031768   \n",
       "26126   -0.11205    -0.028606  -0.139983 -0.618441 -0.053906 -0.031768   \n",
       "\n",
       "        flag_S3   flag_SF   flag_SH  \n",
       "28282 -0.019726  0.825156 -0.046432  \n",
       "19433 -0.019726  0.825156 -0.046432  \n",
       "30618 -0.019726  0.825156 -0.046432  \n",
       "14731 -0.019726  0.825156 -0.046432  \n",
       "26126 -0.019726  0.825156 -0.046432  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/train_set.csv\"\n",
    ")\n",
    "train_df = pd.read_csv(train_set_path)\n",
    "# get only sample\n",
    "train_df = train_df.sample(frac=0.1, random_state=42)\n",
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assume dataset is already preprocessed and normalized [0,1]\n",
    "# # Load NSL-KDD dataset (example)\n",
    "# # X: features, y: labels (1 for normal, -1 for anomaly)\n",
    "# # This is just a placeholder - you would need to load the actual data\n",
    "\n",
    "# # Filter only normal data for training\n",
    "# X = np.random.rand(1000, 122)  # Placeholder: assume 122 features in NSL-KDD\n",
    "# y = np.random.choice([-1, 1], size=1000)  # Random labels for illustration\n",
    "\n",
    "# # Filter only normal data points for training\n",
    "# X_normal = X[y == 1]\n",
    "# X_train, X_val = train_test_split(X_normal, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.7735\n",
      "Epoch 2/10, Loss: 0.7603\n",
      "Epoch 3/10, Loss: 0.7339\n",
      "Epoch 4/10, Loss: nan\n",
      "Epoch 5/10, Loss: nan\n",
      "Epoch 6/10, Loss: nan\n",
      "Epoch 7/10, Loss: nan\n",
      "Epoch 8/10, Loss: nan\n",
      "Epoch 9/10, Loss: nan\n",
      "Epoch 10/10, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "# X_val_tensor = torch.FloatTensor(X_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize the autoencoder\n",
    "input_dim = X_train.shape[1]\n",
    "autoencoder = DeepSparseDenoisingAutoencoder(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dims=[64, 32, 16],\n",
    "    activation=\"relu\",\n",
    "    dropout_rate=0.2,\n",
    "    noise_factor=0.1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = train_autoencoder(autoencoder, train_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (5387, 122)\n",
      "Bottleneck representation shape: (5387, 16)\n"
     ]
    }
   ],
   "source": [
    "# Extract bottleneck features for OCSVM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_bottleneck = (\n",
    "    autoencoder.get_bottleneck_representation(X_train_tensor.to(device)).cpu().numpy()\n",
    ")\n",
    "\n",
    "print(f\"Original data shape: {X_train.shape}\")\n",
    "print(f\"Bottleneck representation shape: {X_train_bottleneck.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate reconstruction error\n",
    "# autoencoder.eval()\n",
    "# with torch.no_grad():\n",
    "#     # For normal validation data\n",
    "#     # We already have X_val_tensor as our normal validation data\n",
    "#     # since we filtered X_normal before splitting into train/val\n",
    "#     reconstructed_normal, _ = autoencoder(X_val_tensor.to(device))\n",
    "#     reconstruction_error_normal = torch.mean(\n",
    "#         torch.pow(X_val_tensor.to(device) - reconstructed_normal, 2), dim=1\n",
    "#     )\n",
    "\n",
    "#     # Create anomaly validation data separately\n",
    "#     X_anomaly = X[y == -1]  # Get all anomaly data\n",
    "#     X_val_anomaly_tensor = torch.FloatTensor(\n",
    "#         X_anomaly[:100]\n",
    "#     )  # Take at most 100 samples\n",
    "\n",
    "#     reconstructed_anomaly, _ = autoencoder(X_val_anomaly_tensor.to(device))\n",
    "#     reconstruction_error_anomaly = torch.mean(\n",
    "#         torch.pow(X_val_anomaly_tensor.to(device) - reconstructed_anomaly, 2), dim=1\n",
    "#     )\n",
    "\n",
    "#     print(\n",
    "#         f\"Avg Reconstruction Error (Normal): {torch.mean(reconstruction_error_normal):.4f}\"\n",
    "#     )\n",
    "#     print(\n",
    "#         f\"Avg Reconstruction Error (Anomaly): {torch.mean(reconstruction_error_anomaly):.4f}\"\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
