{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "sample_size = 0.01\n",
    "use_sample = True\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "onnx_path = \"autoencoder.onnx\"\n",
    "\n",
    "trained_model_path = \"saved_models/autoencoder_Model_1_hidden[96, 64]_latent55_best.pth\"\n",
    "existing_model_architecture = {\n",
    "    \"input_dim\": 122,\n",
    "    \"hidden_dims\": [96, 64],\n",
    "    \"latent_dim\": 55,\n",
    "    \"activation_type\": \"LeakyReLU\",\n",
    "    \"output_activation_type\": \"Sigmoid\",\n",
    "}\n",
    "\n",
    "train_autoencoder_new = False\n",
    "\n",
    "new_model_architecture = {\n",
    "    \"hidden_dims\": [96, 64],\n",
    "    \"latent_dim\": 55,\n",
    "    \"activation_type\": \"LeakyReLU\",\n",
    "    \"output_activation_type\": \"Sigmoid\",\n",
    "}\n",
    "\n",
    "new_model_learning_parameters = {\n",
    "    \"batch_size\": 64,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"num_epochs\": 100,\n",
    "    \"improvement_threshold\": 0.000000001,\n",
    "    \"good_model_threshold\": 0.00015,\n",
    "    \"early_stopping_patience\": 10,\n",
    "}\n",
    "\n",
    "train_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/train_set_full.csv\"\n",
    ")\n",
    "test_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/test_set.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_parameters = {\n",
    "    \"eps\": 3.307695409141457,\n",
    "    \"min_samples\": 12,\n",
    "    \"distance_metric\": \"manhattan\",\n",
    "    \"score\": 0.32888421416282654,\n",
    "}\n",
    "\n",
    "tree_alogrithm_parameter = \"kd_tree\"\n",
    "\n",
    "dbocsvm_parameter_list = {\n",
    "    0: {\"kernel\": \"rbf\", \"gamma\": 0.007687147387619229, \"nu\": 0.40444202940920865},\n",
    "    1: {\"kernel\": \"rbf\", \"gamma\": 0.0038039544695999085, \"nu\": 0.017822451052202516},\n",
    "    2: {\"kernel\": \"rbf\", \"gamma\": 0.9791363174438797, \"nu\": 0.2118583725675878},\n",
    "    3: {\"kernel\": \"rbf\", \"gamma\": 0.4914145055154016, \"nu\": 0.040386151768396016},\n",
    "    4: {\"kernel\": \"rbf\", \"gamma\": 0.00022076253141631167, \"nu\": 0.3130592052259594},\n",
    "    5: {\"kernel\": \"rbf\", \"gamma\": 0.000984283220825162, \"nu\": 0.41655644153130095},\n",
    "    6: {\"kernel\": \"rbf\", \"gamma\": 0.012392027379523591, \"nu\": 0.4467193441608521},\n",
    "    7: {\"kernel\": \"rbf\", \"gamma\": 0.2558451202114683, \"nu\": 0.17633255279969143},\n",
    "    8: {\"kernel\": \"rbf\", \"gamma\": 0.00024087568250343255, \"nu\": 0.12989484794418313},\n",
    "    9: {\"kernel\": \"rbf\", \"gamma\": 0.0010060506417450942, \"nu\": 0.03414041532280349},\n",
    "    10: {\"kernel\": \"rbf\", \"gamma\": 0.595004349072778, \"nu\": 0.45510171738057137},\n",
    "    11: {\"kernel\": \"rbf\", \"gamma\": 0.00013517586551807536, \"nu\": 0.4791747787001657},\n",
    "    12: {\"kernel\": \"rbf\", \"gamma\": 0.0003394157447952476, \"nu\": 0.4718716575908249},\n",
    "    13: {\"kernel\": \"rbf\", \"gamma\": 0.00039384159785362156, \"nu\": 0.07457879584186154},\n",
    "    14: {\"kernel\": \"rbf\", \"gamma\": 0.0007048121122738379, \"nu\": 0.18826540464548136},\n",
    "    15: {\"kernel\": \"rbf\", \"gamma\": 0.005245787346174036, \"nu\": 0.09681211166283199},\n",
    "    16: {\"kernel\": \"rbf\", \"gamma\": 0.0030281166903027967, \"nu\": 0.41811825264121794},\n",
    "    17: {\"kernel\": \"rbf\", \"gamma\": 0.038821472577236484, \"nu\": 0.2394891173574615},\n",
    "    18: {\"kernel\": \"rbf\", \"gamma\": 0.01383090793686792, \"nu\": 0.13159691223293477},\n",
    "    19: {\"kernel\": \"rbf\", \"gamma\": 0.011671175624307474, \"nu\": 0.19686845454924085},\n",
    "    20: {\"kernel\": \"rbf\", \"gamma\": 0.3377966579171783, \"nu\": 0.24170681620666995},\n",
    "    21: {\"kernel\": \"rbf\", \"gamma\": 0.26053601470615245, \"nu\": 0.250858730234716},\n",
    "    22: {\"kernel\": \"rbf\", \"gamma\": 0.013861771161444126, \"nu\": 0.48852768663471235},\n",
    "    23: {\"kernel\": \"rbf\", \"gamma\": 0.0038213452664339286, \"nu\": 0.2597242286087554},\n",
    "    24: {\"kernel\": \"rbf\", \"gamma\": 0.33105339464807937, \"nu\": 0.09329261906603245},\n",
    "    25: {\"kernel\": \"rbf\", \"gamma\": 0.17893775830273984, \"nu\": 0.0677458931085475},\n",
    "    26: {\"kernel\": \"rbf\", \"gamma\": 0.07951818528932965, \"nu\": 0.06339529623447664},\n",
    "    27: {\"kernel\": \"rbf\", \"gamma\": 0.00279768794282823, \"nu\": 0.4133642297303994},\n",
    "    28: {\"kernel\": \"rbf\", \"gamma\": 0.0011767719693130242, \"nu\": 0.25089655311004516},\n",
    "    29: {\"kernel\": \"rbf\", \"gamma\": 0.001942758883167201, \"nu\": 0.19986828167112286},\n",
    "    30: {\"kernel\": \"rbf\", \"gamma\": 0.22008869793121752, \"nu\": 0.058867632267701336},\n",
    "    31: {\"kernel\": \"rbf\", \"gamma\": 0.004279245742687558, \"nu\": 0.3784774647503394},\n",
    "    32: {\"kernel\": \"rbf\", \"gamma\": 0.0006486892775127467, \"nu\": 0.2082359465574189},\n",
    "    33: {\"kernel\": \"rbf\", \"gamma\": 0.015191632862316635, \"nu\": 0.4763401321881779},\n",
    "    34: {\"kernel\": \"rbf\", \"gamma\": 0.00021328218909427393, \"nu\": 0.34598747547535996},\n",
    "    35: {\"kernel\": \"rbf\", \"gamma\": 0.0042188290640615515, \"nu\": 0.16588432532178707},\n",
    "    36: {\"kernel\": \"rbf\", \"gamma\": 0.0029560057421308323, \"nu\": 0.29988216117202604},\n",
    "    37: {\"kernel\": \"rbf\", \"gamma\": 0.0011386243908070707, \"nu\": 0.015432428554398081},\n",
    "    38: {\"kernel\": \"rbf\", \"gamma\": 0.02014907380195306, \"nu\": 0.062031671961285346},\n",
    "    39: {\"kernel\": \"rbf\", \"gamma\": 0.674787995963264, \"nu\": 0.10163167874868935},\n",
    "    40: {\"kernel\": \"rbf\", \"gamma\": 0.0003281288820932392, \"nu\": 0.13654409593073555},\n",
    "    41: {\"kernel\": \"rbf\", \"gamma\": 0.5863189558521061, \"nu\": 0.2355414835373833},\n",
    "    42: {\"kernel\": \"rbf\", \"gamma\": 0.01697546147968382, \"nu\": 0.38407003101091547},\n",
    "    43: {\"kernel\": \"rbf\", \"gamma\": 0.7186806846286208, \"nu\": 0.21014983598533357},\n",
    "    44: {\"kernel\": \"rbf\", \"gamma\": 0.002023099844824229, \"nu\": 0.020974893099220335},\n",
    "    45: {\"kernel\": \"rbf\", \"gamma\": 0.010012930657812967, \"nu\": 0.23429139725241815},\n",
    "    46: {\"kernel\": \"rbf\", \"gamma\": 0.00124076096985608, \"nu\": 0.333945159308331},\n",
    "    47: {\"kernel\": \"rbf\", \"gamma\": 0.6718741051468257, \"nu\": 0.0910573417709725},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_set_path)\n",
    "if use_sample:\n",
    "    train_df = train_df.sample(frac=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = train_df.values\n",
    "\n",
    "print(X_train_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_full_tensor = torch.FloatTensor(X_train_full)\n",
    "\n",
    "# Create data loaders\n",
    "X_train_full_dataset = TensorDataset(X_train_full_tensor)\n",
    "\n",
    "input_dim = X_train_full.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train an autoencoder if not existing autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BatchNormAutoencoder\n",
    "from torch import nn, optim\n",
    "\n",
    "if train_autoencoder_new:\n",
    "    autoencoder = BatchNormAutoencoder(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=new_model_architecture[\"hidden_dims\"],\n",
    "        latent_dim=new_model_architecture[\"latent_dim\"],\n",
    "        activation_type=new_model_architecture[\"activation_type\"],\n",
    "        output_activation_type=new_model_architecture[\"output_activation_type\"],\n",
    "    )\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        autoencoder.parameters(), lr=new_model_learning_parameters[\"learning_rate\"]\n",
    "    )\n",
    "    criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if train_autoencoder_new:\n",
    "    X_train, X_val = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "    X_train = X_train.values\n",
    "    X_val = X_val.values\n",
    "\n",
    "    print(X_train.shape, X_val.shape, X_train_full.shape)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    X_val_tensor = torch.FloatTensor(X_val)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor)\n",
    "\n",
    "    input_dim = X_train.shape[1]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=new_model_learning_parameters[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=new_model_learning_parameters[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_autoencoder\n",
    "\n",
    "\n",
    "if train_autoencoder_new:\n",
    "    new_best_model_path = \"best_models/new_model.pth\"\n",
    "\n",
    "    history, is_good_model = train_autoencoder(\n",
    "        model=autoencoder,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epochs=new_model_learning_parameters[\"num_epochs\"],\n",
    "        best_model_path=new_best_model_path,\n",
    "        verbose=True,\n",
    "        early_stopping_patience=new_model_learning_parameters[\n",
    "            \"early_stopping_patience\"\n",
    "        ],\n",
    "        improvement_threshold=new_model_learning_parameters[\"improvement_threshold\"],\n",
    "        good_model_threshold=new_model_learning_parameters[\"good_model_threshold\"],\n",
    "        plot_results=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load trained autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BatchNormAutoencoder\n",
    "\n",
    "if not train_autoencoder_new:\n",
    "    autoencoder = BatchNormAutoencoder(\n",
    "        input_dim=existing_model_architecture[\"input_dim\"],\n",
    "        hidden_dims=existing_model_architecture[\"hidden_dims\"],\n",
    "        latent_dim=existing_model_architecture[\"latent_dim\"],\n",
    "        activation_type=existing_model_architecture[\"activation_type\"],\n",
    "        output_activation_type=existing_model_architecture[\"output_activation_type\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "if train_autoencoder_new:\n",
    "    checkpoint = torch.load(new_best_model_path)\n",
    "    autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    checkpoint = torch.load(trained_model_path)\n",
    "    autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the DBOCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# extract encoded features\n",
    "X_train_full_loader = DataLoader(X_train_full_dataset, batch_size=256)\n",
    "\n",
    "# Extract in batches to prevent memory issues\n",
    "X_train_full_encoded = []\n",
    "with torch.no_grad():\n",
    "    for data in X_train_full_loader:\n",
    "        data_x = data[0].to(device)\n",
    "        encoded = autoencoder.encode(data_x)\n",
    "        X_train_full_encoded.append(encoded.cpu().numpy())\n",
    "X_train_full_encoded = np.vstack(X_train_full_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DBOCSVM_V2\n",
    "\n",
    "dbocsvm = DBOCSVM_V2(\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"auto\",\n",
    "    nu=0.2,\n",
    "    eps=dbscan_parameters[\"eps\"],\n",
    "    min_samples=dbscan_parameters[\"min_samples\"],\n",
    "    dbscan_metric=dbscan_parameters[\"distance_metric\"],\n",
    "    algorithm=tree_alogrithm_parameter,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbocsvm.fit_cluster(X_train_full_encoded, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbocsvm.fit_ocsvm(X_train_full_encoded, parameter_list=dbocsvm_parameter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_set_path)\n",
    "print(test_df.shape)\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into X and y\n",
    "X_test = test_df.drop(\n",
    "    columns=[\"attack_binary\", \"attack_categorical\", \"attack_class\"]\n",
    ").values\n",
    "y_test = test_df[\"attack_binary\"].values\n",
    "y_test_class = test_df[\"attack_class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "X_test_encoded = []\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, torch.zeros(len(X_test_tensor)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        encoded = autoencoder.encode(data)\n",
    "        X_test_encoded.append(encoded.cpu().numpy())\n",
    "\n",
    "X_test_encoded = np.vstack(X_test_encoded)\n",
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dbocsvm.predict(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[-1, 1])\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, title):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=labels, yticklabels=labels\n",
    "    )\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"Actual Label\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_confusion_matrix(cm, [\"Anomaly\", \"Normal\"], \"Confusion Matrix (Anomaly vs Normal)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Anomaly\", \"Normal\"]))\n",
    "\n",
    "precision = precision_score(y_test, y_pred, pos_label=-1)\n",
    "recall = recall_score(y_test, y_pred, pos_label=-1)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=-1)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiclass_cm(y_true_class, y_pred_binary):\n",
    "    \"\"\"\n",
    "    Create a confusion matrix showing how each attack class was classified.\n",
    "\n",
    "    For attack classes (DoS, Probe, R2L, U2R), correct detection is when y_pred = -1 (anomaly)\n",
    "    For normal class, correct detection is when y_pred = 1 (normal)\n",
    "    \"\"\"\n",
    "    classes = np.unique(y_true_class)\n",
    "    cm = np.zeros((len(classes), 2))\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        # Get predictions for this class\n",
    "        cls_indices = y_true_class == cls\n",
    "        preds = y_pred_binary[cls_indices]\n",
    "\n",
    "        # Count correct and incorrect predictions\n",
    "        if cls == \"normal\":\n",
    "            cm[i, 0] = np.sum(preds == -1)  # incorrectly detected as anomaly\n",
    "            cm[i, 1] = np.sum(preds == 1)  # correctly detected as normal\n",
    "        else:\n",
    "            cm[i, 0] = np.sum(preds == -1)  # correctly detected as anomaly\n",
    "            cm[i, 1] = np.sum(preds == 1)  # incorrectly detected as normal\n",
    "\n",
    "    return cm, classes\n",
    "\n",
    "\n",
    "# Create and plot the multi-class confusion matrix\n",
    "cm_multi, classes = create_multiclass_cm(y_test_class, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm_multi,\n",
    "    annot=True,\n",
    "    fmt=\"g\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Detected as Anomaly\", \"Detected as Normal\"],\n",
    "    yticklabels=classes,\n",
    ")\n",
    "plt.ylabel(\"True Attack Class\")\n",
    "plt.title(\"Confusion Matrix by Attack Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate detection rates for each class\n",
    "print(\"Detection rates by class:\")\n",
    "class_metrics = {}\n",
    "for cls in np.unique(y_test_class):\n",
    "    # Get indices for this class\n",
    "    class_indices = y_test_class == cls\n",
    "\n",
    "    # True values and predictions for this class\n",
    "    y_true_cls = y_test[class_indices]\n",
    "    y_pred_cls = y_pred[class_indices]\n",
    "\n",
    "    # Calculate metrics\n",
    "    if cls == \"Normal\":\n",
    "        # For normal class, we want to detect 1 (normal)\n",
    "        correct = np.sum((y_pred_cls == 1))\n",
    "        precision = precision_score(\n",
    "            y_true_cls, y_pred_cls, pos_label=1, zero_division=0\n",
    "        )\n",
    "        recall = recall_score(y_true_cls, y_pred_cls, pos_label=1, zero_division=0)\n",
    "    else:\n",
    "        # For attack classes, we want to detect -1 (anomaly)\n",
    "        correct = np.sum((y_pred_cls == -1))\n",
    "        precision = precision_score(\n",
    "            y_true_cls, y_pred_cls, pos_label=-1, zero_division=0\n",
    "        )\n",
    "        recall = recall_score(y_true_cls, y_pred_cls, pos_label=-1, zero_division=0)\n",
    "\n",
    "    total = len(y_pred_cls)\n",
    "    detection_rate = correct / total\n",
    "    f1 = f1_score(\n",
    "        y_true_cls, y_pred_cls, pos_label=-1 if cls != \"Normal\" else 1, zero_division=0\n",
    "    )\n",
    "\n",
    "    class_metrics[cls] = {\n",
    "        \"detection_rate\": detection_rate,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"count\": total,\n",
    "        \"correctly_detected\": correct,\n",
    "    }\n",
    "\n",
    "    print(f\"{cls}: {detection_rate:.4f} ({correct}/{total})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
