{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with_storage_dbscan = False\n",
    "with_storage_dbocsvm = False\n",
    "os.makedirs(\"optuna_storage\", exist_ok=True)\n",
    "dbscan_optuna_storage_path = \"sqlite:///optuna_storage/dbscan_study.db\"\n",
    "ocsvm_optuna_storage_path = \"sqlite:///optuna_storage/dbocsvm_study.db\"\n",
    "\n",
    "train_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/train_set_full.csv\"\n",
    ")\n",
    "test_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/test_set.csv\"\n",
    ")\n",
    "\n",
    "results_path = \"tuning_results/results_dbocsvm_01.json\"\n",
    "\n",
    "test_run = True\n",
    "\n",
    "if test_run:\n",
    "    sample_size = 0.03\n",
    "    use_sample = True\n",
    "    ocsvm_trials = 5\n",
    "else:\n",
    "    sample_size = 1.0\n",
    "    use_sample = False\n",
    "    ocsvm_trials = 1000\n",
    "\n",
    "\n",
    "use_full_train_set = True\n",
    "\n",
    "dbscan_tuning_parameters = {\n",
    "    \"evaluation_metric\": \"silhouette\",  # silhouette, calinski_harabasz, davies_bouldin\n",
    "    \"distance_metric\": \"manhattan\",  # manhattan, euclidean\n",
    "    \"trials\": 10,\n",
    "}\n",
    "dbocsvm_tree_algorithm = \"kd_tree\"  # \"ball_tree\" or \"kd_tree\"\n",
    "\n",
    "existing_model_path = \"best_models/config 5/autoencoder_Model_1_hidden[96, 64]_latent55_lr0.001_bs128_optadamw_actELU_dr0.1_wd0.pth\"\n",
    "\n",
    "existing_model_architecture = {\n",
    "    \"input_dim\": 122,\n",
    "    \"hidden_dims\": [96, 64],\n",
    "    \"latent_dim\": 55,\n",
    "    \"activation_type\": \"ELU\",\n",
    "    \"negative_slope\": 0.02,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"output_activation_type\": \"Sigmoid\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE\n",
    "override_dbscan_tuning = False\n",
    "dbscan_override_params = {\n",
    "    \"eps\": 0.5,\n",
    "    \"min_samples\": 5,\n",
    "    \"distance_metric\": \"manhattan\",\n",
    "    \"n_clusters\": 5,\n",
    "    \"cluster_data_points\": {\"-1\": 293, \"0\": 66992, \"1\": 57},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020, 122)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_REJ</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.833486e-07</td>\n",
       "      <td>2.572642e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration     src_bytes     dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
       "0       0.0  5.833486e-07  2.572642e-07   0.0             0.0     0.0  0.0   \n",
       "\n",
       "   num_failed_logins  logged_in  num_compromised  ...  flag_REJ  flag_RSTO  \\\n",
       "0                0.0        1.0              0.0  ...       0.0        0.0   \n",
       "\n",
       "   flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  flag_SF  \\\n",
       "0          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "   flag_SH  \n",
       "0      0.0  \n",
       "\n",
       "[1 rows x 122 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_set_path)\n",
    "\n",
    "if use_sample:\n",
    "    train_df = train_df.sample(frac=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1616, 122) (404, 122) (2020, 122)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_full = train_df.values\n",
    "\n",
    "X_train, X_val = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_train_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor)\n",
    "\n",
    "input_dim = X_train_full.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the autoencoder or use existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from models import BatchNormAutoencoderV2\n",
    "\n",
    "autoencoder = BatchNormAutoencoderV2(\n",
    "    input_dim=existing_model_architecture[\"input_dim\"],\n",
    "    hidden_dims=existing_model_architecture[\"hidden_dims\"],\n",
    "    latent_dim=existing_model_architecture[\"latent_dim\"],\n",
    "    activation_type=existing_model_architecture[\"activation_type\"],\n",
    "    negative_slope=existing_model_architecture[\"negative_slope\"],\n",
    "    dropout_rate=existing_model_architecture[\"dropout_rate\"],\n",
    "    output_activation_type=existing_model_architecture[\"output_activation_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNormAutoencoderV2(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=122, out_features=96, bias=True)\n",
       "    (1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=96, out_features=64, bias=True)\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ELU(alpha=1.0)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=64, out_features=55, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=55, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=96, bias=True)\n",
       "    (5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ELU(alpha=1.0)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=96, out_features=122, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(existing_model_path)\n",
    "autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbscan tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# extract encoded features\n",
    "X_train_full_tensor = torch.FloatTensor(X_train_full)\n",
    "X_train_full_dataset = TensorDataset(X_train_full_tensor)\n",
    "X_train_full_loader = DataLoader(X_train_full_dataset, batch_size=256)\n",
    "\n",
    "# Extract in batches to prevent memory issues\n",
    "if use_full_train_set:\n",
    "    X_encoded = []\n",
    "    with torch.no_grad():\n",
    "        for data in X_train_full_loader:\n",
    "            data_x = data[0].to(device)\n",
    "            encoded = autoencoder.encode(data_x)\n",
    "            X_encoded.append(encoded.cpu().numpy())\n",
    "    X_encoded = np.vstack(X_encoded)\n",
    "else:\n",
    "    X_encoded = []\n",
    "    with torch.no_grad():\n",
    "        for data in train_loader:\n",
    "            data_x = data[0].to(device)\n",
    "            encoded = autoencoder.encode(data_x)\n",
    "            X_encoded.append(encoded.cpu().numpy())\n",
    "    X_encoded = np.vstack(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbct/Projects/thesis/db-ocsvm/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import find_eps_range_with_elbow_method\n",
    "\n",
    "input_dim_encoded = X_encoded.shape[1]\n",
    "\n",
    "k_for_elbow = int((20 + input_dim_encoded * 2) / 2)\n",
    "# CHANGE\n",
    "if not override_dbscan_tuning:\n",
    "    min_eps, max_eps = find_eps_range_with_elbow_method(\n",
    "        X_encoded,\n",
    "        k=k_for_elbow,\n",
    "        plot=False,\n",
    "    )\n",
    "    min_eps, max_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:53,563] A new study created in memory with name: no-name-d55daba5-4607-45c0-a2d3-24b27bfa4056\n",
      "[I 2025-03-12 05:09:53,875] Trial 0 finished with value: 0.5637109875679016 and parameters: {'eps': 19.269543744356973, 'min_samples': 89}. Best is trial 0 with value: 0.5637109875679016.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 997, 1: 206, -1: 817}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:54,158] Trial 1 finished with value: 0.5789753794670105 and parameters: {'eps': 15.682731990668852, 'min_samples': 109}. Best is trial 1 with value: 0.5789753794670105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 969, 1: 194, -1: 857}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:54,492] Trial 2 finished with value: 0.29372191429138184 and parameters: {'eps': 10.110531299468047, 'min_samples': 3}. Best is trial 1 with value: 0.5789753794670105.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "{0: 84, 1: 985, 2: 5, 3: 3, 4: 30, 5: 210, 6: 68, 7: 32, 8: 73, 9: 8, 10: 4, 11: 13, 12: 17, 13: 4, 14: 10, 15: 8, 16: 10, 17: 14, 18: 17, 19: 6, 20: 9, 21: 3, 22: 3, 23: 7, 24: 3, 25: 6, 26: 9, 27: 5, 28: 4, 29: 5, 30: 3, 31: 4, 32: 3, 33: 9, 34: 3, 35: 4, 36: 3, 37: 6, 38: 3, 39: 5, 40: 3, 41: 3, 42: 3, 43: 3, -1: 310}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:54,732] Trial 3 finished with value: 0.582797110080719 and parameters: {'eps': 12.030707220608225, 'min_samples': 38}. Best is trial 3 with value: 0.582797110080719.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 956, 1: 190, -1: 874}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:54,963] Trial 4 finished with value: 0.593856692314148 and parameters: {'eps': 10.70385898524255, 'min_samples': 101}. Best is trial 4 with value: 0.593856692314148.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 917, 1: 121, -1: 982}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:55,196] Trial 5 finished with value: 0.5871599316596985 and parameters: {'eps': 7.258377560212691, 'min_samples': 12}. Best is trial 4 with value: 0.593856692314148.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{0: 920, 1: 186, 2: 25, 3: 15, -1: 874}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:55,460] Trial 6 finished with value: 0.5608515739440918 and parameters: {'eps': 19.965497747791662, 'min_samples': 80}. Best is trial 4 with value: 0.593856692314148.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 1000, 1: 211, -1: 809}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:55,679] Trial 7 finished with value: 0.652356743812561 and parameters: {'eps': 6.2558061395139255, 'min_samples': 45}. Best is trial 7 with value: 0.652356743812561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{0: 400, 1: 457, 2: 85, -1: 1078}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:55,959] Trial 8 finished with value: 0.4493907690048218 and parameters: {'eps': 16.960051140036814, 'min_samples': 6}. Best is trial 7 with value: 0.652356743812561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "{0: 100, 1: 1005, 2: 82, 3: 16, 4: 53, 5: 32, 6: 6, 7: 216, 8: 93, 9: 12, 10: 8, 11: 9, 12: 13, 13: 31, 14: 10, 15: 8, 16: 12, 17: 20, 18: 7, 19: 13, 20: 6, -1: 268}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:56,277] Trial 9 finished with value: -0.05791078880429268 and parameters: {'eps': 8.415041377064306, 'min_samples': 1}. Best is trial 7 with value: 0.652356743812561.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n",
      "{0: 80, 1: 945, 2: 2, 3: 2, 4: 1, 5: 1, 6: 1, 7: 202, 8: 1, 9: 5, 10: 2, 11: 29, 12: 1, 13: 2, 14: 1, 15: 1, 16: 63, 17: 4, 18: 1, 19: 1, 20: 29, 21: 1, 22: 61, 23: 1, 24: 2, 25: 1, 26: 1, 27: 5, 28: 2, 29: 8, 30: 2, 31: 4, 32: 1, 33: 3, 34: 1, 35: 7, 36: 13, 37: 1, 38: 1, 39: 1, 40: 4, 41: 10, 42: 8, 43: 1, 44: 2, 45: 1, 46: 1, 47: 1, 48: 10, 49: 2, 50: 1, 51: 2, 52: 1, 53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 2, 63: 2, 64: 11, 65: 1, 66: 1, 67: 1, 68: 6, 69: 7, 70: 20, 71: 1, 72: 1, 73: 5, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 2, 83: 1, 84: 2, 85: 1, 86: 3, 87: 1, 88: 1, 89: 1, 90: 1, 91: 1, 92: 2, 93: 1, 94: 1, 95: 1, 96: 2, 97: 2, 98: 1, 99: 1, 100: 1, 101: 7, 102: 1, 103: 1, 104: 3, 105: 1, 106: 6, 107: 1, 108: 1, 109: 1, 110: 1, 111: 1, 112: 1, 113: 1, 114: 1, 115: 1, 116: 1, 117: 1, 118: 1, 119: 6, 120: 1, 121: 1, 122: 2, 123: 1, 124: 1, 125: 1, 126: 1, 127: 4, 128: 1, 129: 2, 130: 1, 131: 1, 132: 1, 133: 4, 134: 1, 135: 1, 136: 1, 137: 1, 138: 1, 139: 1, 140: 2, 141: 1, 142: 11, 143: 1, 144: 3, 145: 1, 146: 1, 147: 1, 148: 2, 149: 1, 150: 4, 151: 1, 152: 1, 153: 1, 154: 2, 155: 1, 156: 1, 157: 2, 158: 1, 159: 1, 160: 2, 161: 1, 162: 4, 163: 1, 164: 1, 165: 1, 166: 2, 167: 2, 168: 1, 169: 1, 170: 3, 171: 1, 172: 2, 173: 1, 174: 1, 175: 1, 176: 1, 177: 3, 178: 1, 179: 3, 180: 1, 181: 2, 182: 4, 183: 1, 184: 1, 185: 1, 186: 3, 187: 1, 188: 1, 189: 2, 190: 2, 191: 1, 192: 1, 193: 1, 194: 1, 195: 1, 196: 1, 197: 2, 198: 1, 199: 1, 200: 6, 201: 1, 202: 1, 203: 1, 204: 1, 205: 4, 206: 1, 207: 1, 208: 1, 209: 1, 210: 1, 211: 1, 212: 1, 213: 1, 214: 1, 215: 1, 216: 1, 217: 6, 218: 1, 219: 1, 220: 1, 221: 1, 222: 1, 223: 2, 224: 1, 225: 1, 226: 1, 227: 2, 228: 1, 229: 3, 230: 1, 231: 1, 232: 1, 233: 1, 234: 1, 235: 1, 236: 1, 237: 1, 238: 1, 239: 5, 240: 1, 241: 1, 242: 2, 243: 2, 244: 2, 245: 1, 246: 4, 247: 1, 248: 1, 249: 1, 250: 1, 251: 3, 252: 1, 253: 1, 254: 1, 255: 1, 256: 1, 257: 2, 258: 1, 259: 1, 260: 1, 261: 1, 262: 1, 263: 1, 264: 1, 265: 1, 266: 1, 267: 1, 268: 1, 269: 1, 270: 1, 271: 1, 272: 1, 273: 1, 274: 1, 275: 1, 276: 2, 277: 3, 278: 1, 279: 1, 280: 1, 281: 1, 282: 2, 283: 2, 284: 1, 285: 1, 286: 1, 287: 1, 288: 1, 289: 1, 290: 1, 291: 1, 292: 1, 293: 1, 294: 1, 295: 1, 296: 1, 297: 1, 298: 1, 299: 1, 300: 1, 301: 1, 302: 1, 303: 1, 304: 1, 305: 1, 306: 1, 307: 1, 308: 1, 309: 1, 310: 1, 311: 1, 312: 1, 313: 1, 314: 1, 315: 1, 316: 1, 317: 1, 318: 1, 319: 1, 320: 1, 321: 1, 322: 1, 323: 1, 324: 1, 325: 2, 326: 1, 327: 1, 328: 1, 329: 1, 330: 1, 331: 1, 332: 1, 333: 2, 334: 1, 335: 1, 336: 1, 337: 1, 338: 1, 339: 1, 340: 1, 341: 1, 342: 1, 343: 1, 344: 1, 345: 1, 346: 1, 347: 1, 348: 1, 349: 1, 350: 1, 351: 1, 352: 1, 353: 1, 354: 1, 355: 1, 356: 1, 357: 1, 358: 1, 359: 1, 360: 1, 361: 1, 362: 1, 363: 1, 364: 1, 365: 1, 366: 1, 367: 1, 368: 1, 369: 1, 370: 1, 371: 1, 372: 1, 373: 1, 374: 1, 375: 1, 376: 1, 377: 1, 378: 1, 379: 1, 380: 1, 381: 1}\n"
     ]
    }
   ],
   "source": [
    "from utils import objective_dbscan\n",
    "import optuna\n",
    "\n",
    "dbscan_objective_lambda = lambda trial: objective_dbscan(\n",
    "    trial,\n",
    "    X_encoded=X_encoded,\n",
    "    evaluation_metric=dbscan_tuning_parameters[\"evaluation_metric\"],\n",
    "    eps_range=(min_eps, max_eps),\n",
    "    min_samples_range=(1, input_dim_encoded * 2),\n",
    "    distance_metric=dbscan_tuning_parameters[\"distance_metric\"],\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "if with_storage_dbscan:\n",
    "    dbscan_study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        storage=dbscan_optuna_storage_path,\n",
    "        study_name=\"dbscan_study\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    dbscan_study.optimize(\n",
    "        dbscan_objective_lambda,\n",
    "        n_trials=dbscan_tuning_parameters[\"trials\"],\n",
    "    )\n",
    "else:\n",
    "    dbscan_study = optuna.create_study(direction=\"maximize\")\n",
    "    dbscan_study.optimize(\n",
    "        dbscan_objective_lambda,\n",
    "        n_trials=dbscan_tuning_parameters[\"trials\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps = 6.2558061395139255\n",
      "min_samples = 45\n",
      "n_clusters = 3\n",
      "cluster_data_points\n",
      "{-1: 1078, 0: 400, 1: 457, 2: 85}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# CHANGE\n",
    "if override_dbscan_tuning:\n",
    "    eps = dbscan_override_params[\"eps\"]\n",
    "    min_samples = dbscan_override_params[\"min_samples\"]\n",
    "else:\n",
    "    eps = dbscan_study.best_params[\"eps\"]\n",
    "    min_samples = dbscan_study.best_params[\"min_samples\"]\n",
    "\n",
    "# get dbscan best trial\n",
    "if override_dbscan_tuning:\n",
    "    n_clusters = dbscan_override_params[\"n_clusters\"]\n",
    "    cluster_data_points = dbscan_override_params[\"cluster_data_points\"]\n",
    "else:\n",
    "    best_trial_dbscan = dbscan_study.best_trial\n",
    "    best_trial_dbscan_user_attrs = best_trial_dbscan.user_attrs\n",
    "\n",
    "    n_clusters = best_trial_dbscan_user_attrs[\"n_clusters\"]\n",
    "    cluster_data_points = best_trial_dbscan_user_attrs[\"cluster_data_points\"]\n",
    "    \n",
    "\n",
    "print(f\"eps = {eps}\")\n",
    "print(f\"min_samples = {min_samples}\")\n",
    "print(f\"n_clusters = {n_clusters}\")\n",
    "print(\"cluster_data_points\")\n",
    "pprint.pprint(cluster_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DBOCSVM_V2\n",
    "\n",
    "# Create DB-OC-SVM model with default ocsvm parameters\n",
    "dbocsvm = DBOCSVM_V2(\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"auto\",\n",
    "    nu=0.2,\n",
    "    eps=eps,\n",
    "    min_samples=min_samples,\n",
    "    dbscan_metric=dbscan_tuning_parameters[\"distance_metric\"],\n",
    "    algorithm=dbocsvm_tree_algorithm,  # ball_tree, kd_tree,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting DBSCAN...\n",
      "DBSCAN Fitted...\n",
      "Unique Clusters: [-1  0  1  2]\n",
      "Cluster Sizes: {-1: 1078, 0: 400, 1: 457, 2: 85}\n"
     ]
    }
   ],
   "source": [
    "dbocsvm.fit_cluster(X_encoded, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22543, 125)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "      <th>attack_binary</th>\n",
       "      <th>attack_categorical</th>\n",
       "      <th>attack_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>neptune</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
       "0       0.0        0.0        0.0   0.0             0.0     0.0  0.0   \n",
       "\n",
       "   num_failed_logins  logged_in  num_compromised  ...  flag_RSTR  flag_S0  \\\n",
       "0                0.0        0.0              0.0  ...        0.0      0.0   \n",
       "\n",
       "   flag_S1  flag_S2  flag_S3  flag_SF  flag_SH  attack_binary  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0             -1   \n",
       "\n",
       "   attack_categorical  attack_class  \n",
       "0             neptune           DoS  \n",
       "\n",
       "[1 rows x 125 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_set_path)\n",
    "print(test_df.shape)\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22543, 122) (22543,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into X and y\n",
    "X_test = test_df.drop(\n",
    "    columns=[\"attack_binary\", \"attack_categorical\", \"attack_class\"]\n",
    ").values\n",
    "y_test = test_df[\"attack_binary\"].values\n",
    "y_test_class = test_df[\"attack_class\"]\n",
    "\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reconstruction error inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal test samples: 9711\n",
      "Anomaly test samples: 12832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'normal_loss': 0.000964479972240596,\n",
       " 'anomaly_loss': 0.010680428181224482,\n",
       " 'loss_difference': 0.009715948208983886}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate normal and anomaly samples from test set\n",
    "X_test_normal = X_test[y_test == 1]\n",
    "X_test_anomaly = X_test[y_test == -1]\n",
    "\n",
    "print(f\"Normal test samples: {X_test_normal.shape[0]}\")\n",
    "print(f\"Anomaly test samples: {X_test_anomaly.shape[0]}\")\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_test_normal_tensor = torch.FloatTensor(X_test_normal).to(device)\n",
    "X_test_anomaly_tensor = torch.FloatTensor(X_test_anomaly).to(device)\n",
    "\n",
    "# Create DataLoaders for test data evaluation\n",
    "normal_test_dataset = TensorDataset(X_test_normal_tensor)\n",
    "anomaly_test_dataset = TensorDataset(X_test_anomaly_tensor)\n",
    "normal_test_loader = DataLoader(normal_test_dataset, batch_size=256, shuffle=False)\n",
    "anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "def calculate_reconstruction_error(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            outputs = model(x)\n",
    "            # Calculate MSE for each sample\n",
    "            loss = criterion(outputs, x)\n",
    "            loss = loss.mean(dim=1)\n",
    "            total_loss += torch.sum(loss).item()\n",
    "            total_samples += x.size(0)\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "# Function to evaluate a model's reconstruction performance\n",
    "def evaluate_model(model):\n",
    "    normal_loss = calculate_reconstruction_error(model, normal_test_loader)\n",
    "    anomaly_loss = calculate_reconstruction_error(model, anomaly_test_loader)\n",
    "    loss_difference = anomaly_loss - normal_loss\n",
    "\n",
    "    return {\n",
    "        \"normal_loss\": normal_loss,\n",
    "        \"anomaly_loss\": anomaly_loss,\n",
    "        \"loss_difference\": loss_difference,\n",
    "    }\n",
    "\n",
    "\n",
    "reconstruction_error = evaluate_model(autoencoder)\n",
    "reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract features from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22543, 55)\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, torch.zeros(len(X_test_tensor)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "X_test_encoded = []\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        encoded = autoencoder.encode(data)\n",
    "        X_test_encoded.append(encoded.cpu().numpy())\n",
    "\n",
    "X_test_encoded = np.vstack(X_test_encoded)\n",
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the ocsvms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:09:57,623] A new study created in memory with name: no-name-a545d241-fdfa-4a73-9802-8c24d6076529\n",
      "[I 2025-03-12 05:10:00,236] Trial 0 finished with value: 0.7404072217539813 and parameters: {'gamma_0': 0.604611269094318, 'nu_0': 0.4050757831678222, 'gamma_1': 0.8543166422512686, 'nu_1': 0.20280665296236203, 'gamma_2': 0.31813037818859724, 'nu_2': 0.0560313380356262}. Best is trial 0 with value: 0.7404072217539813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': '74.04', 'f1': '81.42', 'precision': '68.70', 'recall': '99.91'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:10:02,992] Trial 1 finished with value: 0.7129042274763785 and parameters: {'gamma_0': 0.2331303744264942, 'nu_0': 0.4215780824416861, 'gamma_1': 0.9519999663309572, 'nu_1': 0.42104527654723367, 'gamma_2': 0.9916322831895353, 'nu_2': 0.23610281851733605}. Best is trial 0 with value: 0.7404072217539813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': '71.29', 'f1': '79.82', 'precision': '66.53', 'recall': '99.73'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:10:05,686] Trial 2 finished with value: 0.7440447145455352 and parameters: {'gamma_0': 0.7107213333113137, 'nu_0': 0.3564683317310839, 'gamma_1': 0.24375856858725567, 'nu_1': 0.27076367814019553, 'gamma_2': 0.34296136339917155, 'nu_2': 0.13007508827654315}. Best is trial 2 with value: 0.7440447145455352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': '74.40', 'f1': '81.63', 'precision': '69.00', 'recall': '99.92'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:10:08,447] Trial 3 finished with value: 0.7411613361132059 and parameters: {'gamma_0': 0.053377347279043835, 'nu_0': 0.29327745908634534, 'gamma_1': 0.9104357264409647, 'nu_1': 0.258967558224491, 'gamma_2': 0.24557585236909718, 'nu_2': 0.09545761092347047}. Best is trial 2 with value: 0.7440447145455352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': '74.12', 'f1': '81.37', 'precision': '68.93', 'recall': '99.28'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 05:10:11,335] Trial 4 finished with value: 0.7451537062502772 and parameters: {'gamma_0': 0.9062955629765135, 'nu_0': 0.29591401455812694, 'gamma_1': 0.4884336671489512, 'nu_1': 0.3006595450044127, 'gamma_2': 0.8766677433631176, 'nu_2': 0.2983960648583581}. Best is trial 4 with value: 0.7451537062502772.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': '74.52', 'f1': '81.69', 'precision': '69.10', 'recall': '99.90'}\n"
     ]
    }
   ],
   "source": [
    "from utils import objective_dbocsvm_fit_ocsvm\n",
    "\n",
    "# Inner Optuna study for DBSCAN\n",
    "dbocsvm_fit_ocsvm_objective_lambda = lambda trial: objective_dbocsvm_fit_ocsvm(\n",
    "    trial,\n",
    "    model=dbocsvm,\n",
    "    X_encoded_train=X_encoded,\n",
    "    X_encoded_test=X_test_encoded,\n",
    "    y_test=y_test,\n",
    "    cluster_count=n_clusters,\n",
    ")\n",
    "\n",
    "if with_storage_dbocsvm:\n",
    "    dbocsvm_study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        storage=ocsvm_optuna_storage_path,\n",
    "        study_name=\"dbocsvm_study\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    dbocsvm_study.optimize(\n",
    "        dbocsvm_fit_ocsvm_objective_lambda,\n",
    "        n_trials=ocsvm_trials,\n",
    "    )\n",
    "else:\n",
    "    dbocsvm_study = optuna.create_study(direction=\"maximize\")\n",
    "    dbocsvm_study.optimize(\n",
    "        dbocsvm_fit_ocsvm_objective_lambda,\n",
    "        n_trials=ocsvm_trials,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list = {}\n",
    "\n",
    "for key, value in dbocsvm_study.best_params.items():\n",
    "    cluster = key.split(\"_\")[1]\n",
    "    cluster = int(cluster)\n",
    "\n",
    "    parameter_list[cluster] = {\n",
    "        \"kernel\": \"rbf\",\n",
    "        \"gamma\": dbocsvm_study.best_params[f\"gamma_{cluster}\"],\n",
    "        \"nu\": dbocsvm_study.best_params[f\"nu_{cluster}\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best parameters and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best autoencoder model:\n",
      "{'input_dim': 122,\n",
      " 'hidden_dims': [96, 64],\n",
      " 'latent_dim': 55,\n",
      " 'activation_type': 'ELU',\n",
      " 'negative_slope': 0.02,\n",
      " 'dropout_rate': 0.1,\n",
      " 'output_activation_type': 'Sigmoid',\n",
      " 'val_loss': 0.00011326836558407006}\n",
      "\n",
      "Reconstruction error:\n",
      "{'normal_loss': 0.000964479972240596,\n",
      " 'anomaly_loss': 0.010680428181224482,\n",
      " 'loss_difference': 0.009715948208983886}\n",
      "\n",
      "Best dbscan parameters\n",
      "{'eps': 6.2558061395139255,\n",
      " 'min_samples': 45,\n",
      " 'distance_metric': 'manhattan',\n",
      " 'evaluation_metric': 'silhouette',\n",
      " 'score': 0.652356743812561,\n",
      " 'n_clusters': 3,\n",
      " 'cluster_data_points': {0: 400, 1: 457, 2: 85, -1: 1078}}\n",
      "\n",
      "Best ocsvm parameters\n",
      "Tree algorithm: kd_tree\n",
      "Accuracy: 0.7451537062502772\n",
      "{0: {'kernel': 'rbf', 'gamma': 0.9062955629765135, 'nu': 0.29591401455812694},\n",
      " 1: {'kernel': 'rbf', 'gamma': 0.4884336671489512, 'nu': 0.3006595450044127},\n",
      " 2: {'kernel': 'rbf', 'gamma': 0.8766677433631176, 'nu': 0.2983960648583581}}\n"
     ]
    }
   ],
   "source": [
    "autoencoder_architecture = {\n",
    "    \"input_dim\": existing_model_architecture[\"input_dim\"],\n",
    "    \"hidden_dims\": existing_model_architecture[\"hidden_dims\"],\n",
    "    \"latent_dim\": existing_model_architecture[\"latent_dim\"],\n",
    "    \"activation_type\": existing_model_architecture[\"activation_type\"],\n",
    "    \"negative_slope\": existing_model_architecture[\"negative_slope\"],\n",
    "    \"dropout_rate\": existing_model_architecture[\"dropout_rate\"],\n",
    "    \"output_activation_type\": existing_model_architecture[\"output_activation_type\"],\n",
    "    \"val_loss\": checkpoint[\"val_loss\"],\n",
    "}\n",
    "\n",
    "print(\"Best autoencoder model:\")\n",
    "pprint.pprint(autoencoder_architecture, sort_dicts=False)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reconstruction error:\")\n",
    "pprint.pprint(reconstruction_error, sort_dicts=False)\n",
    "print(\"\")\n",
    "\n",
    "best_dbscan_parameters = {\n",
    "    \"eps\": eps,\n",
    "    \"min_samples\": min_samples,\n",
    "    \"distance_metric\": dbscan_tuning_parameters[\"distance_metric\"],\n",
    "    \"evaluation_metric\": dbscan_tuning_parameters[\"evaluation_metric\"],\n",
    "    \"score\": best_trial_dbscan.value,\n",
    "    \"n_clusters\": n_clusters,\n",
    "    \"cluster_data_points\": cluster_data_points,\n",
    "}\n",
    "\n",
    "print(\"Best dbscan parameters\")\n",
    "pprint.pprint(best_dbscan_parameters, sort_dicts=False)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Best ocsvm parameters\")\n",
    "print(f\"Tree algorithm: {dbocsvm_tree_algorithm}\")\n",
    "print(f\"Accuracy: {dbocsvm_study.best_value}\")\n",
    "pprint.pprint(parameter_list, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tuning_result = {\n",
    "    \"dbscan\": best_dbscan_parameters,\n",
    "    \"ocsvm\": {\n",
    "        \"tree_algorithm\": dbocsvm_tree_algorithm,\n",
    "        \"accuracy\": dbocsvm_study.best_value,\n",
    "        \"parameters\": parameter_list,\n",
    "    },\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"max_score\": 0,\n",
    "    \"autoencoder_architecture\": autoencoder_architecture,\n",
    "    \"reconstruction_error\": reconstruction_error,\n",
    "    \"tuning_results\": {},\n",
    "}\n",
    "\n",
    "os.makedirs(\"tuning_results\", exist_ok=True)\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, \"r\") as file:\n",
    "        existing_results = json.load(file)\n",
    "        if existing_results[\"max_score\"] < dbocsvm_study.best_value:\n",
    "            with open(results_path, \"w\") as f:\n",
    "                existing_results[\"max_score\"] = dbocsvm_study.best_value\n",
    "                tuning_result_id = len(existing_results[\"tuning_results\"])\n",
    "                tuning_result[\"score\"] = dbocsvm_study.best_value\n",
    "                existing_results[\"tuning_results\"][tuning_result_id] = tuning_result\n",
    "                json.dump(existing_results, f)\n",
    "else:\n",
    "    with open(results_path, \"w\") as f:\n",
    "        results[\"max_score\"] = dbocsvm_study.best_value\n",
    "        tuning_result[\"score\"] = dbocsvm_study.best_value\n",
    "        results[\"tuning_results\"][0] = tuning_result\n",
    "        json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
