{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with_storage_dbscan = False\n",
    "with_storage_dbocsvm = False\n",
    "dbscan_optuna_storage_path = \"sqlite:///optuna_storage/dbscan_study.db\"\n",
    "ocsvm_optuna_storage_path = \"sqlite:///optuna_storage/dbocsvm_study.db\"\n",
    "\n",
    "sample_size = 0.01\n",
    "use_sample = True\n",
    "best_model_path = \"best_models/best_model_proposed.pth\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "export_model = False\n",
    "onnx_path = \"autoencoder.onnx\"\n",
    "\n",
    "use_existing_model = True\n",
    "existing_model_path = \"saved_models/val_score_pca/autoencoder_pca_Model_pca_1_pca_hidden[40, 25, 17]_latent8_best.pth\"\n",
    "existing_model_architecture = {\n",
    "    \"input_dim\": 68,\n",
    "    \"hidden_dims\": [40, 25, 17],\n",
    "    \"latent_dim\": 8,\n",
    "    \"activation_type\": \"LeakyReLU\",\n",
    "    \"output_activation_type\": \"Sigmoid\",\n",
    "}\n",
    "\n",
    "train_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/train_set_full.csv\"\n",
    ")\n",
    "test_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/test_set.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model architecture\n",
    "hidden_dims = [56, 32, 16]\n",
    "latent_dim = 4\n",
    "\n",
    "# Learning parameters\n",
    "lr = 0.001\n",
    "batch_size = 128\n",
    "\n",
    "epochs = 10\n",
    "improvement_threshold = 0.0001\n",
    "good_model_threshold = 0.00015\n",
    "early_stopping_patience = 5\n",
    "\n",
    "dbscan_evaluation_metric = \"silhouette\"  # \"davies_bouldin\" or \"calinski_harabasz\"\n",
    "dbscan_tuning_distance_metric = \"euclidean\"  # \"euclidean\" or \"cosine\" or \"manhattan\"\n",
    "\n",
    "# Used by the DBOCSVM clustering\n",
    "dbocsvm_dbscan_distance_metric = \"euclidean\"  # \"euclidean or \"cosine\" or \"manhattan\"\n",
    "dbocsvm_tree_algorithm = \"kd_tree\"  # \"ball_tree\" or \"kd_tree\"\n",
    "\n",
    "# Used for DBOCSVM ocsvm tuning\n",
    "ocsvm_trials = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(673, 122)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_REJ</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.833486e-07</td>\n",
       "      <td>2.572642e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration     src_bytes     dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
       "0       0.0  5.833486e-07  2.572642e-07   0.0             0.0     0.0  0.0   \n",
       "\n",
       "   num_failed_logins  logged_in  num_compromised  ...  flag_REJ  flag_RSTO  \\\n",
       "0                0.0        1.0              0.0  ...       0.0        0.0   \n",
       "\n",
       "   flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  flag_SF  \\\n",
       "0          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "   flag_SH  \n",
       "0      0.0  \n",
       "\n",
       "[1 rows x 122 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_set_path)\n",
    "if use_sample:\n",
    "    train_df = train_df.sample(frac=sample_size, random_state=42).reset_index(drop=True)\n",
    "print(train_df.shape)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(538, 68) (135, 68) (673, 122)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "X_train_full = train_df.values\n",
    "\n",
    "X_train, X_val = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "autoencoder_training_pca = PCA(n_components=68)\n",
    "\n",
    "X_train = autoencoder_training_pca.fit_transform(X_train.values)\n",
    "X_val = autoencoder_training_pca.transform(X_val.values)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_train_full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import BatchNormAutoencoder\n",
    "from torch import nn, optim\n",
    "\n",
    "if use_existing_model:\n",
    "    autoencoder = BatchNormAutoencoder(\n",
    "        input_dim=existing_model_architecture[\"input_dim\"],\n",
    "        hidden_dims=existing_model_architecture[\"hidden_dims\"],\n",
    "        latent_dim=existing_model_architecture[\"latent_dim\"],\n",
    "        activation_type=existing_model_architecture[\"activation_type\"],\n",
    "        output_activation_type=existing_model_architecture[\"output_activation_type\"],\n",
    "    )\n",
    "else:\n",
    "    # Create model\n",
    "    autoencoder = BatchNormAutoencoder(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=hidden_dims,\n",
    "        latent_dim=latent_dim,\n",
    "        activation_type=\"LeakyReLU\",\n",
    "        output_activation_type=\"Sigmoid\",\n",
    "    )\n",
    "\n",
    "    # loss and optimizer\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_existing_model:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbct/Projects/thesis/db-ocsvm/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import train_autoencoder\n",
    "\n",
    "if not use_existing_model:\n",
    "    history, is_good_model = train_autoencoder(\n",
    "        model=autoencoder,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        epochs=epochs,\n",
    "        best_model_path=best_model_path,\n",
    "        verbose=True,\n",
    "        early_stopping_patience=early_stopping_patience,\n",
    "        improvement_threshold=improvement_threshold,\n",
    "        good_model_threshold=good_model_threshold,\n",
    "        plot_results=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNormAutoencoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=68, out_features=40, bias=True)\n",
       "    (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Linear(in_features=40, out_features=25, bias=True)\n",
       "    (4): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=25, out_features=17, bias=True)\n",
       "    (7): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "    (9): Linear(in_features=17, out_features=8, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=17, bias=True)\n",
       "    (1): BatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Linear(in_features=17, out_features=25, bias=True)\n",
       "    (4): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.01)\n",
       "    (6): Linear(in_features=25, out_features=40, bias=True)\n",
       "    (7): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.01)\n",
       "    (9): Linear(in_features=40, out_features=68, bias=True)\n",
       "    (10): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "if use_existing_model:\n",
    "    checkpoint = torch.load(existing_model_path)\n",
    "    autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "else:\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "if export_model and not use_existing_model:\n",
    "    # Prepare a sample input tensor with the correct shape\n",
    "    dummy_input = torch.randn(1, input_dim, device=device)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        autoencoder,  # model being run\n",
    "        dummy_input,  # model input\n",
    "        onnx_path,  # where to save the model\n",
    "        export_params=True,  # store trained parameters inside model file\n",
    "        opset_version=17,  # ONNX version\n",
    "        do_constant_folding=True,  # optimize constant folding\n",
    "        input_names=[\"input\"],  # model's input names\n",
    "        output_names=[\"output\"],  # model's output names\n",
    "        dynamic_axes={\n",
    "            \"input\": {0: \"batch_size\"},  # variable length axes\n",
    "            \"output\": {0: \"batch_size\"},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    print(f\"Model exported to ONNX format: {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbscan tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "dbocsvm_pca = PCA(n_components=68)\n",
    "X_train_full = dbocsvm_pca.fit_transform(X_train_full)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_full_tensor = torch.FloatTensor(X_train_full)\n",
    "\n",
    "# Create data loaders\n",
    "X_train_full_dataset = TensorDataset(X_train_full_tensor)\n",
    "\n",
    "# create data loader\n",
    "X_train_full_loader = DataLoader(X_train_full_dataset, batch_size=128)\n",
    "\n",
    "input_dim = X_train_full.shape[1]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(673, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract in batches to prevent memory issues\n",
    "X_encoded_full = []\n",
    "with torch.no_grad():\n",
    "    for data in X_train_full_loader:\n",
    "        data_x = data[0].to(device)\n",
    "        encoded = autoencoder.encode(data_x)\n",
    "        X_encoded_full.append(encoded.cpu().numpy())\n",
    "X_encoded_full = np.vstack(X_encoded_full)\n",
    "X_encoded_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(1.2888195337727666), np.float64(5.155278135091066))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import find_eps_range_with_elbow_method\n",
    "\n",
    "input_dim_encoded = X_encoded_full.shape[1]\n",
    "\n",
    "k_for_elbow = int((20 + input_dim_encoded * 2) / 2)\n",
    "min_eps, max_eps = find_eps_range_with_elbow_method(\n",
    "    X_encoded_full,\n",
    "    k=k_for_elbow,\n",
    "    plot=False,\n",
    ")\n",
    "\n",
    "min_eps, max_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:47:30,375] A new study created in memory with name: no-name-c1e4cf74-9440-4ba2-b8f1-446e8521a3c9\n",
      "[I 2025-03-06 18:47:30,429] Trial 0 finished with value: -inf and parameters: {'eps': 3.7781914607207128, 'min_samples': 13}. Best is trial 0 with value: -inf.\n",
      "[I 2025-03-06 18:47:30,472] Trial 1 finished with value: -inf and parameters: {'eps': 3.756273791900667, 'min_samples': 7}. Best is trial 0 with value: -inf.\n",
      "[I 2025-03-06 18:47:30,564] Trial 2 finished with value: -inf and parameters: {'eps': 1.5447075460072976, 'min_samples': 12}. Best is trial 0 with value: -inf.\n",
      "[I 2025-03-06 18:47:30,615] Trial 3 finished with value: -inf and parameters: {'eps': 3.7462394668731895, 'min_samples': 1}. Best is trial 0 with value: -inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough clusters\n",
      "not enough clusters\n",
      "not enough clusters\n",
      "not enough clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:47:30,657] Trial 4 finished with value: -inf and parameters: {'eps': 2.9957571735077497, 'min_samples': 9}. Best is trial 0 with value: -inf.\n",
      "[I 2025-03-06 18:47:30,696] Trial 5 finished with value: -inf and parameters: {'eps': 4.64618732949849, 'min_samples': 6}. Best is trial 0 with value: -inf.\n",
      "[I 2025-03-06 18:47:30,718] Trial 6 finished with value: -inf and parameters: {'eps': 4.8580466573356595, 'min_samples': 12}. Best is trial 0 with value: -inf.\n",
      "[I 2025-03-06 18:47:30,760] Trial 7 finished with value: -inf and parameters: {'eps': 3.6358249034036376, 'min_samples': 1}. Best is trial 0 with value: -inf.\n",
      "[I 2025-03-06 18:47:30,806] Trial 8 finished with value: -inf and parameters: {'eps': 4.9871553130448625, 'min_samples': 2}. Best is trial 0 with value: -inf.\n",
      "[I 2025-03-06 18:47:30,827] Trial 9 finished with value: -inf and parameters: {'eps': 4.081148383578258, 'min_samples': 11}. Best is trial 0 with value: -inf.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not enough clusters\n",
      "not enough clusters\n",
      "not enough clusters\n",
      "not enough clusters\n",
      "not enough clusters\n",
      "not enough clusters\n"
     ]
    }
   ],
   "source": [
    "from utils import objective_dbscan\n",
    "import optuna\n",
    "\n",
    "dbscan_objective_lambda = lambda trial: objective_dbscan(\n",
    "    trial,\n",
    "    X_encoded=X_encoded_full,\n",
    "    evaluation_metric=dbscan_evaluation_metric,\n",
    "    eps_range=(min_eps, max_eps),\n",
    "    min_samples_range=(1, input_dim_encoded * 2),\n",
    "    distance_metric=dbscan_tuning_distance_metric,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "dbscan_trials = 10\n",
    "\n",
    "if with_storage_dbscan:\n",
    "    dbscan_study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        storage=dbscan_optuna_storage_path,\n",
    "        study_name=\"dbscan_study\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    dbscan_study.optimize(\n",
    "        dbscan_objective_lambda,\n",
    "        n_trials=dbscan_trials,\n",
    "    )\n",
    "else:\n",
    "    dbscan_study = optuna.create_study(direction=\"maximize\")\n",
    "    dbscan_study.optimize(\n",
    "        dbscan_objective_lambda,\n",
    "        n_trials=dbscan_trials,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps = 3.7781914607207128\n",
      "min_samples = 13\n",
      "n_clusters = 1\n",
      "cluster_data_points\n",
      "{0: 673}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# get dbscan best parameters\n",
    "eps = dbscan_study.best_params[\"eps\"]\n",
    "min_samples = dbscan_study.best_params[\"min_samples\"]\n",
    "\n",
    "# get dbscan best trial\n",
    "best_trial_dbscan = dbscan_study.best_trial\n",
    "best_trial_dbscan_user_attrs = best_trial_dbscan.user_attrs\n",
    "\n",
    "n_clusters = best_trial_dbscan_user_attrs[\"n_clusters\"]\n",
    "cluster_data_points = best_trial_dbscan_user_attrs[\"cluster_data_points\"]\n",
    "\n",
    "print(f\"eps = {eps}\")\n",
    "print(f\"min_samples = {min_samples}\")\n",
    "print(f\"n_clusters = {n_clusters}\")\n",
    "print(\"cluster_data_points\")\n",
    "pprint.pprint(cluster_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import DBOCSVM_V2\n",
    "\n",
    "# Create DB-OC-SVM model with default ocsvm parameters\n",
    "dbocsvm = DBOCSVM_V2(\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"auto\",\n",
    "    nu=0.2,\n",
    "    eps=eps,\n",
    "    min_samples=min_samples,\n",
    "    dbscan_metric=dbocsvm_dbscan_distance_metric,\n",
    "    algorithm=dbocsvm_tree_algorithm,  # ball_tree, kd_tree,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting DBSCAN...\n",
      "DBSCAN Fitted...\n",
      "Unique Clusters: [0]\n",
      "Cluster Sizes: {0: 673}\n"
     ]
    }
   ],
   "source": [
    "dbocsvm.fit_cluster(X_encoded_full, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22543, 125)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "      <th>attack_binary</th>\n",
       "      <th>attack_categorical</th>\n",
       "      <th>attack_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>neptune</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
       "0       0.0        0.0        0.0   0.0             0.0     0.0  0.0   \n",
       "\n",
       "   num_failed_logins  logged_in  num_compromised  ...  flag_RSTR  flag_S0  \\\n",
       "0                0.0        0.0              0.0  ...        0.0      0.0   \n",
       "\n",
       "   flag_S1  flag_S2  flag_S3  flag_SF  flag_SH  attack_binary  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0             -1   \n",
       "\n",
       "   attack_categorical  attack_class  \n",
       "0             neptune           DoS  \n",
       "\n",
       "[1 rows x 125 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_set_path)\n",
    "print(test_df.shape)\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22543, 68), (22543,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting into X and y\n",
    "X_test = test_df.drop(\n",
    "    columns=[\"attack_binary\", \"attack_categorical\", \"attack_class\"]\n",
    ").values\n",
    "\n",
    "X_test = dbocsvm_pca.transform(X_test)\n",
    "\n",
    "y_test = test_df[\"attack_binary\"].values\n",
    "y_test_class = test_df[\"attack_class\"]\n",
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract features from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22543, 8)\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "X_test_encoded = []\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, torch.zeros(len(X_test_tensor)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        encoded = autoencoder.encode(data)\n",
    "        X_test_encoded.append(encoded.cpu().numpy())\n",
    "\n",
    "X_test_encoded = np.vstack(X_test_encoded)\n",
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the ocsvms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-06 18:47:31,640] A new study created in memory with name: no-name-f56e45ca-7356-4510-b6cc-160bd673bc8a\n",
      "[I 2025-03-06 18:47:34,867] Trial 0 finished with value: 0.74337931952269 and parameters: {'gamma_0': 0.25676691717207983, 'nu_0': 0.2478138055508375}. Best is trial 0 with value: 0.74337931952269.\n",
      "[I 2025-03-06 18:47:37,176] Trial 1 finished with value: 0.5677593931597391 and parameters: {'gamma_0': 0.01539200028913066, 'nu_0': 0.19603369436221424}. Best is trial 0 with value: 0.74337931952269.\n",
      "[I 2025-03-06 18:47:39,455] Trial 2 finished with value: 0.6302621656390011 and parameters: {'gamma_0': 0.11968653850944348, 'nu_0': 0.08597888060137186}. Best is trial 0 with value: 0.74337931952269.\n",
      "[I 2025-03-06 18:47:41,589] Trial 3 finished with value: 0.5921572106640642 and parameters: {'gamma_0': 0.04891441511385903, 'nu_0': 0.037458129824031655}. Best is trial 0 with value: 0.74337931952269.\n",
      "[I 2025-03-06 18:47:43,842] Trial 4 finished with value: 0.5891851128953556 and parameters: {'gamma_0': 0.00025164497060568814, 'nu_0': 0.13037741072992115}. Best is trial 0 with value: 0.74337931952269.\n",
      "[I 2025-03-06 18:47:46,321] Trial 5 finished with value: 0.5597746528855965 and parameters: {'gamma_0': 0.0086661638226112, 'nu_0': 0.31763851178754976}. Best is trial 0 with value: 0.74337931952269.\n",
      "[I 2025-03-06 18:47:48,510] Trial 6 finished with value: 0.7667125049904626 and parameters: {'gamma_0': 0.5698829485600022, 'nu_0': 0.06324164848666507}. Best is trial 6 with value: 0.7667125049904626.\n",
      "[I 2025-03-06 18:47:50,920] Trial 7 finished with value: 0.5777846781706073 and parameters: {'gamma_0': 0.05195193526825881, 'nu_0': 0.3672002426978292}. Best is trial 6 with value: 0.7667125049904626.\n",
      "[I 2025-03-06 18:47:53,248] Trial 8 finished with value: 0.551035798252229 and parameters: {'gamma_0': 0.0004130050712649871, 'nu_0': 0.26159394129304103}. Best is trial 6 with value: 0.7667125049904626.\n",
      "[I 2025-03-06 18:47:55,554] Trial 9 finished with value: 0.5567138357805084 and parameters: {'gamma_0': 0.0006176918079494831, 'nu_0': 0.23962052001893508}. Best is trial 6 with value: 0.7667125049904626.\n"
     ]
    }
   ],
   "source": [
    "from utils import objective_dbocsvm_fit_ocsvm\n",
    "\n",
    "# Inner Optuna study for DBSCAN\n",
    "dbocsvm_fit_ocsvm_objective_lambda = lambda trial: objective_dbocsvm_fit_ocsvm(\n",
    "    trial,\n",
    "    model=dbocsvm,\n",
    "    X_encoded_train=X_encoded_full,\n",
    "    X_encoded_test=X_test_encoded,\n",
    "    y_test=y_test,\n",
    "    cluster_count=n_clusters,\n",
    ")\n",
    "\n",
    "if with_storage_dbocsvm:\n",
    "    dbocsvm_study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        storage=ocsvm_optuna_storage_path,\n",
    "        study_name=\"dbocsvm_study\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    dbocsvm_study.optimize(\n",
    "        dbocsvm_fit_ocsvm_objective_lambda,\n",
    "        n_trials=ocsvm_trials,\n",
    "    )\n",
    "else:\n",
    "    dbocsvm_study = optuna.create_study(direction=\"maximize\")\n",
    "    dbocsvm_study.optimize(\n",
    "        dbocsvm_fit_ocsvm_objective_lambda,\n",
    "        n_trials=ocsvm_trials,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list = {}\n",
    "\n",
    "for key, value in dbocsvm_study.best_params.items():\n",
    "    cluster = key.split(\"_\")[1]\n",
    "    cluster = int(cluster)\n",
    "\n",
    "    parameter_list[cluster] = {\n",
    "        \"kernel\": \"rbf\",\n",
    "        \"gamma\": dbocsvm_study.best_params[f\"gamma_{cluster}\"],\n",
    "        \"nu\": dbocsvm_study.best_params[f\"nu_{cluster}\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best parameters and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best autoencoder model:\n",
      "{'input_dim': 68,\n",
      " 'hidden_dims': [56, 32, 16],\n",
      " 'latent_dim': 4,\n",
      " 'activation_type': 'LeakyReLU',\n",
      " 'output_activation_type': 'Sigmoid',\n",
      " 'learning_rate': 0.001,\n",
      " 'batch_size': 128,\n",
      " 'val_loss': 0.01944045111853278}\n",
      "\n",
      "Best dbscan parameters\n",
      "{'eps': 3.7781914607207128,\n",
      " 'min_samples': 13,\n",
      " 'distance_metric': 'euclidean',\n",
      " 'score': -inf}\n",
      "\n",
      "Best ocsvm parameters\n",
      "Tree algorithm: kd_tree\n",
      "Accuracy: 0.7667125049904626\n",
      "{0: {'kernel': 'rbf', 'gamma': 0.5698829485600022, 'nu': 0.06324164848666507}}\n"
     ]
    }
   ],
   "source": [
    "autoencoder_architecture = {\n",
    "    \"input_dim\": input_dim,\n",
    "    \"hidden_dims\": hidden_dims,\n",
    "    \"latent_dim\": latent_dim,\n",
    "    \"activation_type\": \"LeakyReLU\",\n",
    "    \"output_activation_type\": \"Sigmoid\",\n",
    "    \"learning_rate\": lr,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"val_loss\": checkpoint[\"val_loss\"],\n",
    "}\n",
    "print(\"Best autoencoder model:\")\n",
    "pprint.pprint(autoencoder_architecture, sort_dicts=False)\n",
    "print(\"\")\n",
    "best_dbscan_parameters = {\n",
    "    \"eps\": eps,\n",
    "    \"min_samples\": min_samples,\n",
    "    \"distance_metric\": dbocsvm_dbscan_distance_metric,\n",
    "    \"score\": best_trial_dbscan.value,\n",
    "}\n",
    "print(\"Best dbscan parameters\")\n",
    "pprint.pprint(best_dbscan_parameters, sort_dicts=False)\n",
    "print(\"\")\n",
    "print(\"Best ocsvm parameters\")\n",
    "print(f\"Tree algorithm: {dbocsvm_tree_algorithm}\")\n",
    "print(f\"Accuracy: {dbocsvm_study.best_value}\")\n",
    "pprint.pprint(parameter_list, sort_dicts=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
