{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "with_storage_dbscan = False\n",
    "os.makedirs(\"optuna_storage\", exist_ok=True)\n",
    "dbscan_optuna_storage_path = \"sqlite:///optuna_storage/dbscan_study.db\"\n",
    "\n",
    "train_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/train_set_full.csv\"\n",
    ")\n",
    "test_set_path = (\n",
    "    \"/home/jbct/Projects/thesis/db-ocsvm/data/processed/NSL-KDD/test_set.csv\"\n",
    ")\n",
    "\n",
    "results_path = \"tuning_results/results_01.json\"\n",
    "\n",
    "test_run = True\n",
    "\n",
    "if test_run:\n",
    "    sample_size = 0.03\n",
    "    use_sample = True\n",
    "else:\n",
    "    sample_size = 1.0\n",
    "    use_sample = False\n",
    "\n",
    "\n",
    "use_full_train_set = True\n",
    "\n",
    "dbscan_tuning_parameters = {\n",
    "    \"evaluation_metric\": \"silhouette\",  # silhouette, calinski_harabasz, davies_bouldin\n",
    "    \"distance_metric\": \"manhattan\",  # manhattan, euclidean\n",
    "    \"trials\": 10,\n",
    "}\n",
    "\n",
    "existing_model_path = \"best_models/config 5/autoencoder_Model_1_hidden[96, 64]_latent55_lr0.001_bs128_optadamw_actELU_dr0.1_wd0.pth\"\n",
    "\n",
    "existing_model_architecture = {\n",
    "    \"input_dim\": 122,\n",
    "    \"hidden_dims\": [96, 64],\n",
    "    \"latent_dim\": 55,\n",
    "    \"activation_type\": \"ELU\",\n",
    "    \"negative_slope\": 0.02,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"output_activation_type\": \"Sigmoid\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2020, 122)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_REJ</th>\n",
       "      <th>flag_RSTO</th>\n",
       "      <th>flag_RSTOS0</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.833486e-07</td>\n",
       "      <td>2.572642e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration     src_bytes     dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
       "0       0.0  5.833486e-07  2.572642e-07   0.0             0.0     0.0  0.0   \n",
       "\n",
       "   num_failed_logins  logged_in  num_compromised  ...  flag_REJ  flag_RSTO  \\\n",
       "0                0.0        1.0              0.0  ...       0.0        0.0   \n",
       "\n",
       "   flag_RSTOS0  flag_RSTR  flag_S0  flag_S1  flag_S2  flag_S3  flag_SF  \\\n",
       "0          0.0        0.0      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "   flag_SH  \n",
       "0      0.0  \n",
       "\n",
       "[1 rows x 122 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_set_path)\n",
    "\n",
    "if use_sample:\n",
    "    train_df = train_df.sample(frac=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1616, 122) (404, 122) (2020, 122)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_full = train_df.values\n",
    "\n",
    "X_train, X_val = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_val = X_val.values\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_train_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_val_tensor = torch.FloatTensor(X_val)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor)\n",
    "\n",
    "input_dim = X_train_full.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the autoencoder or use existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class BatchNormAutoencoderV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 128,\n",
    "        hidden_dims: List[int] = [64, 32],\n",
    "        latent_dim: int = 16,\n",
    "        activation_type: str = \"ReLU\",\n",
    "        negative_slope: float = 0.2,\n",
    "        dropout_rate: float = 0.2,  # Added dropout\n",
    "        output_activation_type: Optional[str] = \"Sigmoid\",  # Default for [0,1] data\n",
    "    ) -> None:\n",
    "        super(BatchNormAutoencoderV2, self).__init__()\n",
    "\n",
    "        # Select activation function\n",
    "        activation: nn.Module\n",
    "        if activation_type == \"ReLU\":\n",
    "            activation = nn.ReLU()\n",
    "        elif activation_type == \"LeakyReLU\":\n",
    "            activation = nn.LeakyReLU(negative_slope)  # Better negative gradient\n",
    "        elif activation_type == \"ELU\":\n",
    "            activation = nn.ELU()\n",
    "        elif activation_type == \"GELU\":  # Adding GELU option\n",
    "            activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation type provided\")\n",
    "\n",
    "        # Build encoder\n",
    "        encoder_layers: List[nn.Module] = []\n",
    "        current_dim: int = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(current_dim, h_dim))\n",
    "            encoder_layers.append(nn.BatchNorm1d(h_dim))\n",
    "            encoder_layers.append(activation)\n",
    "            encoder_layers.append(nn.Dropout(dropout_rate))  # Add dropout\n",
    "            current_dim = h_dim\n",
    "\n",
    "        # Latent layer\n",
    "        encoder_layers.append(nn.Linear(current_dim, latent_dim))\n",
    "        self.encoder: nn.Sequential = nn.Sequential(*encoder_layers)\n",
    "\n",
    "        # Select output activation function\n",
    "        output_activation: Optional[nn.Module] = None\n",
    "        if output_activation_type == \"ReLU\":\n",
    "            output_activation = nn.ReLU()\n",
    "        elif output_activation_type == \"LeakyReLU\":\n",
    "            output_activation = nn.LeakyReLU(0.2)\n",
    "        elif output_activation_type == \"ELU\":\n",
    "            output_activation = nn.ELU()\n",
    "        elif output_activation_type == \"Sigmoid\":\n",
    "            output_activation = nn.Sigmoid()\n",
    "        elif output_activation_type == \"Tanh\":\n",
    "            output_activation = nn.Tanh()\n",
    "        elif output_activation_type is None:\n",
    "            output_activation = None\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation type provided\")\n",
    "\n",
    "        # Build decoder\n",
    "        decoder_layers: List[nn.Module] = []\n",
    "        current_dim = latent_dim\n",
    "        for h_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(nn.Linear(current_dim, h_dim))\n",
    "            decoder_layers.append(nn.BatchNorm1d(h_dim))\n",
    "            decoder_layers.append(activation)\n",
    "            decoder_layers.append(nn.Dropout(dropout_rate))  # Add dropout\n",
    "            current_dim = h_dim\n",
    "\n",
    "        # Add final output layer (no batch norm on output layer)\n",
    "        decoder_layers.append(nn.Linear(current_dim, input_dim))\n",
    "\n",
    "        # Add output activation if specified\n",
    "        if output_activation is not None:\n",
    "            decoder_layers.append(output_activation)\n",
    "\n",
    "        self.decoder: nn.Sequential = nn.Sequential(*decoder_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        encoded: torch.Tensor = self.encoder(x)\n",
    "        decoded: torch.Tensor = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "autoencoder = BatchNormAutoencoderV2(\n",
    "    input_dim=existing_model_architecture[\"input_dim\"],\n",
    "    hidden_dims=existing_model_architecture[\"hidden_dims\"],\n",
    "    latent_dim=existing_model_architecture[\"latent_dim\"],\n",
    "    activation_type=existing_model_architecture[\"activation_type\"],\n",
    "    negative_slope=existing_model_architecture[\"negative_slope\"],\n",
    "    dropout_rate=existing_model_architecture[\"dropout_rate\"],\n",
    "    output_activation_type=existing_model_architecture[\"output_activation_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNormAutoencoderV2(\n",
       "  (encoder): Sequential(\n",
       "    (0): Linear(in_features=122, out_features=96, bias=True)\n",
       "    (1): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=96, out_features=64, bias=True)\n",
       "    (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ELU(alpha=1.0)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=64, out_features=55, bias=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): Linear(in_features=55, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=96, bias=True)\n",
       "    (5): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ELU(alpha=1.0)\n",
       "    (7): Dropout(p=0.1, inplace=False)\n",
       "    (8): Linear(in_features=96, out_features=122, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(existing_model_path)\n",
    "autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "autoencoder.to(device)\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbscan tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# extract encoded features\n",
    "X_train_full_tensor = torch.FloatTensor(X_train_full)\n",
    "X_train_full_dataset = TensorDataset(X_train_full_tensor)\n",
    "X_train_full_loader = DataLoader(X_train_full_dataset, batch_size=256)\n",
    "\n",
    "if use_full_train_set:\n",
    "    X_encoded = []\n",
    "    with torch.no_grad():\n",
    "        for data in X_train_full_loader:\n",
    "            data_x = data[0].to(device)\n",
    "            encoded = autoencoder.encode(data_x)\n",
    "            X_encoded.append(encoded.cpu().numpy())\n",
    "    X_encoded = np.vstack(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def find_eps_range_with_elbow_method(X, k=20, multiplier=(0.5, 2.0), plot=True):\n",
    "    \"\"\"\n",
    "    Find a suitable eps range for DBSCAN using the elbow method.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        The encoded data points\n",
    "    k : int, default=20\n",
    "        Number of neighbors to consider (corresponds to min_samples)\n",
    "    multiplier : tuple, default=(0.5, 2.0)\n",
    "        Factors to multiply the elbow point by to create a range\n",
    "    plot : bool, default=True\n",
    "        Whether to show the k-distance plot\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (min_eps, max_eps) suitable range for eps parameter\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate distances to k nearest neighbors for each point\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric=\"manhattan\").fit(X)\n",
    "    distances, _ = nbrs.kneighbors(X)\n",
    "\n",
    "    # Sort the distances to the kth neighbor in ascending order\n",
    "    k_distances = np.sort(distances[:, -1])\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(k_distances)\n",
    "        plt.xlabel(\"Points (sorted)\")\n",
    "        plt.ylabel(f\"Distance to {k}th nearest neighbor\")\n",
    "        plt.title(\"K-distance Plot for DBSCAN eps Selection\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add horizontal lines at suggested eps range\n",
    "        # This will be calculated below\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Find the elbow point (simple method - you might want a more sophisticated approach)\n",
    "    # Look for maximum curvature in the sorted distance plot\n",
    "    n_points = len(k_distances)\n",
    "    all_coords = np.vstack((range(n_points), k_distances)).T\n",
    "\n",
    "    # Compute point-to-line distances for all points\n",
    "    line_vec = all_coords[-1] - all_coords[0]\n",
    "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "    vec_from_first = all_coords - all_coords[0]\n",
    "    scalar_prod = np.sum(vec_from_first * np.tile(line_vec_norm, (n_points, 1)), axis=1)\n",
    "    vec_to_line = vec_from_first - np.outer(scalar_prod, line_vec_norm)\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line**2, axis=1))\n",
    "\n",
    "    # Elbow point is the point with max distance to the line\n",
    "    elbow_idx = np.argmax(dist_to_line)\n",
    "    elbow_eps = k_distances[elbow_idx]\n",
    "\n",
    "    # Create a range around the elbow point\n",
    "    min_eps = elbow_eps * multiplier[0]\n",
    "    max_eps = elbow_eps * multiplier[1]\n",
    "\n",
    "    if plot:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(k_distances)\n",
    "        plt.axhline(\n",
    "            y=min_eps,\n",
    "            color=\"r\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.7,\n",
    "            label=f\"Min eps: {min_eps:.2f}\",\n",
    "        )\n",
    "        plt.axhline(\n",
    "            y=elbow_eps,\n",
    "            color=\"g\",\n",
    "            linestyle=\"-\",\n",
    "            alpha=0.7,\n",
    "            label=f\"Elbow eps: {elbow_eps:.2f}\",\n",
    "        )\n",
    "        plt.axhline(\n",
    "            y=max_eps,\n",
    "            color=\"r\",\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.7,\n",
    "            label=f\"Max eps: {max_eps:.2f}\",\n",
    "        )\n",
    "        plt.axvline(x=elbow_idx, color=\"g\", linestyle=\":\", alpha=0.5)\n",
    "        plt.xlabel(\"Points (sorted)\")\n",
    "        plt.ylabel(f\"Distance to {k}th nearest neighbor\")\n",
    "        plt.title(\"K-distance Plot with Suggested eps Range\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    return min_eps, max_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(5.716840744018555), np.float64(22.86736297607422))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim_encoded = X_encoded.shape[1]\n",
    "\n",
    "k_for_elbow = int((20 + input_dim_encoded * 2) / 2)\n",
    "min_eps, max_eps = find_eps_range_with_elbow_method(\n",
    "    X_encoded,\n",
    "    k=k_for_elbow,\n",
    "    plot=False,\n",
    ")\n",
    "min_eps, max_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbct/Projects/thesis/db-ocsvm/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score,\n",
    ")\n",
    "\n",
    "\n",
    "def objective_dbscan(\n",
    "    trial: optuna.Trial,\n",
    "    X_encoded: np.ndarray,\n",
    "    evaluation_metric: str = \"silhouette\",\n",
    "    eps_range: tuple[float, float] = (0.1, 15.0),\n",
    "    min_samples_range: tuple[int, int] = (20, 50),\n",
    "    distance_metric: str = \"euclidean\",  # \"manhattan\", \"cosine\"\n",
    "    n_jobs: int = -1,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Inner objective function for optimizing DBSCAN clustering hyperparameters.\n",
    "\n",
    "    This function is used by Optuna for hyperparameter optimization of a DBSCAN model.\n",
    "    It evaluates different clustering configurations using the specified evaluation metric.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : optuna.Trial\n",
    "        Optuna trial object used for hyperparameter suggestion\n",
    "    X_encoded : np.ndarray\n",
    "        Array of encoded data points to cluster\n",
    "    evaluation_metric : str, default=\"silhouette\"\n",
    "        Metric to evaluate cluster quality: \"silhouette\", \"davies_bouldin\", or \"calinski_harabasz\"\n",
    "    eps_range : tuple[float, float], default=(0.1, 15.0)\n",
    "        Range (min, max) for the eps parameter of DBSCAN\n",
    "    min_samples_range : tuple[int, int], default=(20, 50)\n",
    "        Range (min, max) for the min_samples parameter of DBSCAN\n",
    "    metric : str, default=\"euclidean\"\n",
    "        Distance metric for DBSCAN\n",
    "    score_threshold : float, default=0.60\n",
    "        Minimum score threshold to consider a clustering configuration valid\n",
    "    n_jobs : int, default=-1\n",
    "        Number of parallel jobs to run for DBSCAN\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Clustering quality score (higher is better) unless evaluation_metric is \"davies_bouldin\"\n",
    "        Returns -inf for invalid clustering configurations\n",
    "    \"\"\"\n",
    "\n",
    "    def get_score(X, labels, metric_name, mask=None):\n",
    "        if mask is not None:\n",
    "            X = X[mask]\n",
    "            labels = labels[mask]\n",
    "\n",
    "        if metric_name == \"silhouette\":\n",
    "            return silhouette_score(X, labels)\n",
    "        elif metric_name == \"davies_bouldin\":\n",
    "            return -davies_bouldin_score(\n",
    "                X, labels\n",
    "            )  # Negative because we want to maximize\n",
    "        elif metric_name == \"calinski_harabasz\":\n",
    "            return calinski_harabasz_score(X, labels)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric_name}\")\n",
    "\n",
    "    eps = trial.suggest_float(\"eps\", eps_range[0], eps_range[1])\n",
    "    min_samples = trial.suggest_int(\n",
    "        \"min_samples\", min_samples_range[0], min_samples_range[1]\n",
    "    )\n",
    "\n",
    "    dbscan = DBSCAN(\n",
    "        eps=eps, min_samples=min_samples, metric=distance_metric, n_jobs=n_jobs\n",
    "    )\n",
    "\n",
    "    cluster_labels = dbscan.fit_predict(X_encoded)\n",
    "\n",
    "    # Calculate the number of clusters (excluding noise points)\n",
    "    unique_clusters = set(cluster_labels)\n",
    "    n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
    "    print(n_clusters)\n",
    "    # Set custom attribute for number of clusters\n",
    "    trial.set_user_attr(\"n_clusters\", n_clusters)\n",
    "\n",
    "    # Set custom attribute for cluster data points\n",
    "    cluster_data_points = {}\n",
    "    for cluster_id in unique_clusters:\n",
    "        # Store indices of points in each cluster\n",
    "        cluster_indices = np.where(cluster_labels == cluster_id)[0].tolist()\n",
    "        cluster_data_points[int(cluster_id)] = len(cluster_indices)\n",
    "    trial.set_user_attr(\"cluster_data_points\", cluster_data_points)\n",
    "    print(cluster_data_points)\n",
    "\n",
    "    if n_clusters < 2:\n",
    "        print(\"not enough clusters\")\n",
    "        return -float(\"inf\")  # Penalize solutions with too few clusters\n",
    "\n",
    "    # For silhouette score, we need to exclude noise points (-1)\n",
    "    if evaluation_metric == \"silhouette\":\n",
    "        mask = cluster_labels != -1\n",
    "        if sum(mask) < 2:\n",
    "            print(\"not enough points in clusters\")\n",
    "            return -float(\"inf\")\n",
    "        score = get_score(X_encoded, cluster_labels, evaluation_metric, mask)\n",
    "    else:\n",
    "        score = get_score(X_encoded, cluster_labels, evaluation_metric)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:07,767] A new study created in memory with name: no-name-54a9ff14-04da-4b25-9daf-ba3b7d982981\n",
      "[I 2025-03-12 04:29:07,999] Trial 0 finished with value: 0.5313279628753662 and parameters: {'eps': 19.56210762289765, 'min_samples': 59}. Best is trial 0 with value: 0.5313279628753662.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "{0: 998, 1: 212, 2: 72, -1: 738}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:08,218] Trial 1 finished with value: 0.5955008864402771 and parameters: {'eps': 9.648089974241913, 'min_samples': 66}. Best is trial 1 with value: 0.5955008864402771.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 912, 1: 138, -1: 970}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:08,486] Trial 2 finished with value: 0.5866093635559082 and parameters: {'eps': 12.407587636002894, 'min_samples': 98}. Best is trial 1 with value: 0.5955008864402771.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 950, 1: 158, -1: 912}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:08,745] Trial 3 finished with value: 0.5974699258804321 and parameters: {'eps': 9.038845050942326, 'min_samples': 70}. Best is trial 3 with value: 0.5974699258804321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 898, 1: 126, -1: 996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:08,966] Trial 4 finished with value: 0.5969823002815247 and parameters: {'eps': 7.482672967427734, 'min_samples': 67}. Best is trial 3 with value: 0.5974699258804321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 877, 1: 97, -1: 1046}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:09,250] Trial 5 finished with value: 0.38779646158218384 and parameters: {'eps': 11.628356844093933, 'min_samples': 4}. Best is trial 3 with value: 0.5974699258804321.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "{0: 84, 1: 994, 2: 5, 3: 31, 4: 212, 5: 83, 6: 4, 7: 77, 8: 43, 9: 8, 10: 13, 11: 18, 12: 4, 13: 10, 14: 8, 15: 11, 16: 17, 17: 6, 18: 7, 19: 6, 20: 10, 21: 5, 22: 4, 23: 6, 24: 9, 25: 4, 26: 5, 27: 4, 28: 4, -1: 328}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:09,487] Trial 6 finished with value: 0.6009919047355652 and parameters: {'eps': 7.745111403504765, 'min_samples': 40}. Best is trial 6 with value: 0.6009919047355652.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 890, 1: 135, -1: 995}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:09,700] Trial 7 finished with value: 0.5950215458869934 and parameters: {'eps': 10.243945571271546, 'min_samples': 86}. Best is trial 6 with value: 0.6009919047355652.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 912, 1: 132, -1: 976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:09,941] Trial 8 finished with value: 0.5499024987220764 and parameters: {'eps': 22.115938462489606, 'min_samples': 94}. Best is trial 6 with value: 0.6009919047355652.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{0: 1019, 1: 214, -1: 787}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-12 04:29:10,226] Trial 9 finished with value: 0.4436343014240265 and parameters: {'eps': 16.22089235907331, 'min_samples': 3}. Best is trial 6 with value: 0.6009919047355652.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "{0: 100, 1: 1007, 2: 82, 3: 3, 4: 19, 5: 3, 6: 215, 7: 56, 8: 17, 9: 32, 10: 6, 11: 94, 12: 12, 13: 3, 14: 8, 15: 9, 16: 13, 17: 35, 18: 4, 19: 10, 20: 8, 21: 12, 22: 7, 23: 3, 24: 3, 25: 4, 26: 3, 27: 7, 28: 4, 29: 3, 30: 13, 31: 8, 32: 3, 33: 4, 34: 3, 35: 3, 36: 5, 37: 4, 38: 3, -1: 192}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "dbscan_objective_lambda = lambda trial: objective_dbscan(\n",
    "    trial,\n",
    "    X_encoded=X_encoded,\n",
    "    evaluation_metric=dbscan_tuning_parameters[\"evaluation_metric\"],\n",
    "    eps_range=(min_eps, max_eps),\n",
    "    min_samples_range=(1, input_dim_encoded * 2),\n",
    "    distance_metric=dbscan_tuning_parameters[\"distance_metric\"],\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "if with_storage_dbscan:\n",
    "    dbscan_study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        storage=dbscan_optuna_storage_path,\n",
    "        study_name=\"dbscan_study\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    dbscan_study.optimize(\n",
    "        dbscan_objective_lambda,\n",
    "        n_trials=dbscan_tuning_parameters[\"trials\"],\n",
    "    )\n",
    "else:\n",
    "    dbscan_study = optuna.create_study(direction=\"maximize\")\n",
    "    dbscan_study.optimize(\n",
    "        dbscan_objective_lambda,\n",
    "        n_trials=dbscan_tuning_parameters[\"trials\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps = 7.745111403504765\n",
      "min_samples = 40\n",
      "n_clusters = 2\n",
      "cluster_data_points\n",
      "{-1: 995, 0: 890, 1: 135}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "# get dbscan best parameters\n",
    "eps = dbscan_study.best_params[\"eps\"]\n",
    "min_samples = dbscan_study.best_params[\"min_samples\"]\n",
    "\n",
    "# get dbscan best trial\n",
    "best_trial_dbscan = dbscan_study.best_trial\n",
    "best_trial_dbscan_user_attrs = best_trial_dbscan.user_attrs\n",
    "\n",
    "n_clusters = best_trial_dbscan_user_attrs[\"n_clusters\"]\n",
    "cluster_data_points = best_trial_dbscan_user_attrs[\"cluster_data_points\"]\n",
    "\n",
    "print(f\"eps = {eps}\")\n",
    "print(f\"min_samples = {min_samples}\")\n",
    "print(f\"n_clusters = {n_clusters}\")\n",
    "print(\"cluster_data_points\")\n",
    "pprint.pprint(cluster_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBOCSVM_V2:\n",
    "    def __init__(\n",
    "        self,\n",
    "        kernel=\"rbf\",\n",
    "        gamma=\"scale\",\n",
    "        nu=0.5,\n",
    "        eps=0.5,\n",
    "        min_samples=10,\n",
    "        tree_metric=\"euclidean\",\n",
    "        dbscan_metric=\"euclidean\",\n",
    "        algorithm=\"kd_tree\",  # or 'ball_tree'\n",
    "        n_jobs=-1,  # Add n_jobs parameter\n",
    "    ):\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.nu = nu\n",
    "        self.algorithm = algorithm\n",
    "        self.tree_metric = tree_metric\n",
    "        self.dbscan_metric = dbscan_metric\n",
    "        self.dbscan = DBSCAN(\n",
    "            eps=eps, min_samples=min_samples, n_jobs=n_jobs, metric=dbscan_metric\n",
    "        )  # Make it so that it can accept a metric parameter\n",
    "        self.svms = {}  # One SVM per cluster\n",
    "        self.dbscan_centroids = {}  # To store cluster centroids\n",
    "        self.cluster_points = {}  # Store points in each cluster\n",
    "        self.tree = None\n",
    "        # These attributes are mainly used for inspection purposes\n",
    "        self.cluster_sizes = {}  # Number of points in each cluster\n",
    "        self.n_jobs = n_jobs  # Store n_jobs\n",
    "        self.cluster_labels = None\n",
    "        self.unique_clusters = None\n",
    "\n",
    "    def fit_cluster(\n",
    "        self,\n",
    "        X,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Training data\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "        \"\"\"\n",
    "        NOTE: Current DBSCAN only uses euclidean distance, so the metric parameter is not used\n",
    "        TODO: Add metric parameter to DBSCAN to handle different distance metrics\n",
    "        'euclidean': Standard Euclidean distance. This is the default metric.\n",
    "        'manhattan': Manhattan or L1 distance (sum of absolute differences).\n",
    "        'chebyshev': Chebyshev or maximum distance.\n",
    "        'minkowski': Minkowski distance, a generalization of Euclidean and Manhattan distance. The power parameter p of the Minkowski metric can be controlled by the p parameter of DBSCAN.\n",
    "        'wminkowski': Weighted Minkowski distance.\n",
    "        'seuclidean': Standardized Euclidean distance.\n",
    "        'mahalanobis': Mahalanobis distance.\n",
    "        \"\"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Fitting DBSCAN...\")\n",
    "        # NOTE: we use the dbscan that was initialized in the constructor\n",
    "        self.cluster_labels = self.dbscan.fit_predict(X)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"DBSCAN Fitted...\")\n",
    "\n",
    "        self.unique_clusters = np.unique(self.cluster_labels)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Unique Clusters: {self.unique_clusters}\")\n",
    "\n",
    "        for cluster in self.unique_clusters:\n",
    "            n_points = np.sum(self.cluster_labels == cluster)\n",
    "            self.cluster_sizes[int(cluster)] = int(n_points)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Cluster Sizes: {self.cluster_sizes}\")\n",
    "\n",
    "    def fit_ocsvm(\n",
    "        self,\n",
    "        X,\n",
    "        parameter_list=None,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Training data\n",
    "        parameter_list: dictionary of dictionaries\n",
    "            Each key in the dictionary is the cluster number and\n",
    "            the value is a dictionary containing the parameters for OCSVM\n",
    "            each dictionary looks like this:\n",
    "            {\n",
    "                0 : {\n",
    "                kernel: rbf, linear, poly, or sigmoid,\n",
    "                gamma: 'scale', 'auto' or a float,\n",
    "                nu: a float between 0 and 1 e.g 0.2,\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "        X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "        if parameter_list is None:\n",
    "            raise ValueError(\"parameter_list cannot be None\")\n",
    "\n",
    "        if len(parameter_list) < len(self.unique_clusters) - 1:\n",
    "            raise ValueError(\n",
    "                \"Number of parameters should be equal or greater than the number of clusters\"\n",
    "            )\n",
    "\n",
    "        def filter_dict(original_dict, keys_to_keep):\n",
    "            return {k: original_dict[k] for k in keys_to_keep if k in original_dict}\n",
    "\n",
    "        if len(parameter_list) >= len(self.unique_clusters) - 1:\n",
    "            cluster_count = list(self.cluster_sizes.keys())\n",
    "            if -1 in cluster_count:\n",
    "                cluster_count.remove(-1)\n",
    "            cluster_count\n",
    "\n",
    "            parameter_list = filter_dict(parameter_list, cluster_count)\n",
    "\n",
    "        for cluster in self.unique_clusters:\n",
    "\n",
    "            if cluster == -1:  # Skip noise cluster for SVM training\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Training for cluster {cluster} with {self.cluster_sizes[cluster]} points\"\n",
    "                )\n",
    "\n",
    "            # Boolean masking to get points in the current cluster\n",
    "            points = X[self.cluster_labels == cluster]\n",
    "            self.cluster_points[cluster] = points\n",
    "\n",
    "            if len(points) > 0:\n",
    "                # use parameters defined in constructor if not provided\n",
    "                if parameter_list is None:\n",
    "                    ocsvm = OneClassSVM(\n",
    "                        kernel=self.kernel,\n",
    "                        nu=self.nu,\n",
    "                        gamma=self.gamma,\n",
    "                    )\n",
    "                else:\n",
    "                    ocsvm = OneClassSVM(\n",
    "                        kernel=parameter_list[cluster][\"kernel\"],\n",
    "                        nu=parameter_list[cluster][\"nu\"],\n",
    "                        gamma=parameter_list[cluster][\"gamma\"],\n",
    "                    )\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            f\"OCSVM for cluster {cluster} uses nu: {parameter_list[cluster]['nu']}, gamma: {parameter_list[cluster]['gamma']}, kernel: {parameter_list[cluster]['kernel']}\"\n",
    "                        )\n",
    "\n",
    "                ocsvm.fit(points)\n",
    "\n",
    "                self.svms[cluster] = ocsvm\n",
    "\n",
    "                \"\"\"\n",
    "                TODO: Explore other alternatives for centroid calculation\n",
    "                \"->\" means the following line might be a downside of the current approach.\n",
    "                \n",
    "                - Median: More robust to outliers than the mean (`np.median(points, axis=0)`).\n",
    "                    -> Less representative if data is asymmetric  \n",
    "                - Trimmed Mean: Removes extreme values before computing the mean (`scipy.stats.trim_mean`).\n",
    "                    ->   Requires choosing the trimming percentage\n",
    "                - Weighted Mean: Assigns importance to points based on reliability.  \n",
    "                    ->  Requires defining weights\n",
    "                - Geometric Median: Minimizes sum of distances to all points. More robust to outliers than the mean.\n",
    "                    -> computationally expensive (`scipy.spatial`)\n",
    "                - Distance Metrics: Use median for Manhattan distance and mean for Euclidean distance.\n",
    "                    -> Requires choosing the distance metric\n",
    "                    \n",
    "                \"\"\"\n",
    "                self.dbscan_centroids[cluster] = np.mean(points, axis=0)\n",
    "\n",
    "        # Build tree with cluster centroids\n",
    "        centroids = [self.dbscan_centroids[c] for c in self.dbscan_centroids if c != -1]\n",
    "        self.valid_clusters = list(self.dbscan_centroids.keys())\n",
    "        if len(centroids) > 0:\n",
    "            centroids = np.array(centroids)\n",
    "            if self.algorithm == \"kd_tree\":\n",
    "                self.tree = KDTree(centroids, metric=self.tree_metric)\n",
    "            elif self.algorithm == \"ball_tree\":\n",
    "                self.tree = BallTree(centroids, metric=self.tree_metric)\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X,\n",
    "        dbscan_evaluation_metric=\"silhouette\",  # only used for reruns\n",
    "        dbscan_rerun=False,  # only used for reruns\n",
    "        dbscan_rerun_trials=10,  # only used for reruns\n",
    "        parameter_list=None,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like\n",
    "            Training data\n",
    "        dbscan_evaluation_metric : str\n",
    "            Metric to optimize ('silhouette', 'davies_bouldin', or 'calinski_harabasz')\n",
    "        dbscan_rerun : bool\n",
    "            Whether to rerun DBSCAN after fitting the model with the best parameters\n",
    "        dbscan_rerun_trials : int\n",
    "            Number of reruns for DBSCAN after fitting the model with the best parameters\n",
    "        parameter_list: dictionary of dictionaries\n",
    "            Each key in the dictionary is the cluster number and\n",
    "            the value is a dictionary containing the parameters for OCSVM\n",
    "            each dictionary looks like this:\n",
    "            {\n",
    "                0 : {\n",
    "                kernel: rbf, linear, poly, or sigmoid,\n",
    "                gamma: 'scale', 'auto' or a float,\n",
    "                nu: a float between 0 and 1 e.g 0.2,\n",
    "                }\n",
    "            }\n",
    "        \"\"\"\n",
    "\n",
    "        X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "        \"\"\"\n",
    "        NOTE: Current DBSCAN only uses euclidean distance, so the metric parameter is not used\n",
    "        TODO: Add metric parameter to DBSCAN to handle different distance metrics\n",
    "        'euclidean': Standard Euclidean distance. This is the default metric.\n",
    "        'manhattan': Manhattan or L1 distance (sum of absolute differences).\n",
    "        'chebyshev': Chebyshev or maximum distance.\n",
    "        'minkowski': Minkowski distance, a generalization of Euclidean and Manhattan distance. The power parameter p of the Minkowski metric can be controlled by the p parameter of DBSCAN.\n",
    "        'wminkowski': Weighted Minkowski distance.\n",
    "        'seuclidean': Standardized Euclidean distance.\n",
    "        'mahalanobis': Mahalanobis distance.\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(\"Fitting DBSCAN...\")\n",
    "        # NOTE: we use the dbscan that was initialized in the constructor\n",
    "        cluster_labels = self.dbscan.fit_predict(X)\n",
    "        if verbose:\n",
    "            print(\"DBSCAN Fitted...\")\n",
    "\n",
    "        if dbscan_rerun:\n",
    "            if verbose:\n",
    "                print(\"Rerunning DBSCAN...\")\n",
    "\n",
    "            if dbscan_evaluation_metric == \"silhouette\":\n",
    "                current_score = silhouette_score(X, cluster_labels)\n",
    "            elif dbscan_evaluation_metric == \"davies_bouldin\":\n",
    "                current_score = davies_bouldin_score(X, cluster_labels)\n",
    "            else:  # calinski_harabasz\n",
    "                current_score = calinski_harabasz_score(X, cluster_labels)\n",
    "\n",
    "            for i in range(dbscan_rerun_trials):\n",
    "                if verbose:\n",
    "                    print(f\"DBSCAN Rerun {i+1}...\")\n",
    "\n",
    "                new_cluster_labels = self.dbscan.fit_predict(X)\n",
    "\n",
    "                if dbscan_evaluation_metric == \"silhouette\":\n",
    "                    new_score = silhouette_score(X, new_cluster_labels)\n",
    "                    if new_score > current_score:\n",
    "                        cluster_labels = new_cluster_labels\n",
    "                        current_score = new_score\n",
    "                elif dbscan_evaluation_metric == \"davies_bouldin\":\n",
    "                    new_score = davies_bouldin_score(X, new_cluster_labels)\n",
    "                    if new_score < current_score:\n",
    "                        cluster_labels = new_cluster_labels\n",
    "                        current_score = new_score\n",
    "                else:  # calinski_harabasz\n",
    "                    new_score = calinski_harabasz_score(X, new_cluster_labels)\n",
    "                    if new_score > current_score:\n",
    "                        cluster_labels = new_cluster_labels\n",
    "                        current_score = new_score\n",
    "\n",
    "        unique_clusters = np.unique(cluster_labels)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Unique Clusters: {unique_clusters}\")\n",
    "\n",
    "        for cluster in unique_clusters:\n",
    "            # Store the number of points in the cluster\n",
    "            # mainly for inspection purposes\n",
    "            n_points = np.sum(cluster_labels == cluster)\n",
    "            self.cluster_sizes[int(cluster)] = int(n_points)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Cluster Sizes: {self.cluster_sizes}\")\n",
    "\n",
    "        if parameter_list is not None and (len(parameter_list)) < (\n",
    "            len(unique_clusters) - 1\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Number of parameters should be equal or greater than the number of clusters\"\n",
    "            )\n",
    "\n",
    "        def filter_dict(original_dict, keys_to_keep):\n",
    "            return {k: original_dict[k] for k in keys_to_keep if k in original_dict}\n",
    "\n",
    "        if parameter_list is not None and (len(parameter_list)) >= (\n",
    "            len(unique_clusters) - 1\n",
    "        ):\n",
    "            cluster_count = list(self.cluster_sizes.keys())\n",
    "            cluster_count.remove(-1)\n",
    "            cluster_count\n",
    "\n",
    "            parameter_list = filter_dict(parameter_list, cluster_count)\n",
    "\n",
    "        self.parameter_list = parameter_list\n",
    "\n",
    "        for cluster in unique_clusters:\n",
    "\n",
    "            # Store the number of points in the cluster\n",
    "            # n_points = np.sum(cluster_labels == cluster)\n",
    "            # self.cluster_sizes[int(cluster)] = int(n_points)\n",
    "\n",
    "            if cluster == -1:  # Skip noise cluster for SVM training\n",
    "                continue\n",
    "\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"Training for cluster {cluster} with {self.cluster_sizes[cluster]} points\"\n",
    "                )\n",
    "\n",
    "            # Boolean masking to get points in the current cluster\n",
    "            points = X[cluster_labels == cluster]\n",
    "            self.cluster_points[cluster] = points\n",
    "\n",
    "            if len(points) > 0:\n",
    "                # use parameters defined in constructor if not provided\n",
    "                if parameter_list is None:\n",
    "                    ocsvm = OneClassSVM(\n",
    "                        kernel=self.kernel,\n",
    "                        nu=self.nu,\n",
    "                        gamma=self.gamma,\n",
    "                        degree=self.degree,\n",
    "                        coef0=self.coef0,\n",
    "                        tol=self.tol,\n",
    "                        shrinking=self.shrinking,\n",
    "                        cache_size=self.cache_size,\n",
    "                        max_iter=self.max_iter,\n",
    "                    )\n",
    "                else:\n",
    "                    ocsvm = OneClassSVM(\n",
    "                        kernel=parameter_list[cluster][\"kernel\"],\n",
    "                        nu=parameter_list[cluster][\"nu\"],\n",
    "                        gamma=parameter_list[cluster][\"gamma\"],\n",
    "                        degree=self.degree,\n",
    "                        coef0=self.coef0,\n",
    "                        tol=self.tol,\n",
    "                        shrinking=self.shrinking,\n",
    "                        cache_size=self.cache_size,\n",
    "                        max_iter=self.max_iter,\n",
    "                    )\n",
    "                    if verbose:\n",
    "                        print(\n",
    "                            f\"OCSVM for cluster {cluster} uses nu: {parameter_list[cluster]['nu']}, gamma: {parameter_list[cluster]['gamma']}, kernel: {parameter_list[cluster]['kernel']}\"\n",
    "                        )\n",
    "                ocsvm.fit(points)\n",
    "\n",
    "                self.svms[cluster] = ocsvm\n",
    "\n",
    "                \"\"\"\n",
    "                TODO: Explore other alternatives for centroid calculation\n",
    "                \"->\" means the following line might be a downside of the current approach.\n",
    "                \n",
    "                - Median: More robust to outliers than the mean (`np.median(points, axis=0)`).\n",
    "                    -> Less representative if data is asymmetric  \n",
    "                - Trimmed Mean: Removes extreme values before computing the mean (`scipy.stats.trim_mean`).\n",
    "                    ->   Requires choosing the trimming percentage\n",
    "                - Weighted Mean: Assigns importance to points based on reliability.  \n",
    "                    ->  Requires defining weights\n",
    "                - Geometric Median: Minimizes sum of distances to all points. More robust to outliers than the mean.\n",
    "                    -> computationally expensive (`scipy.spatial`)\n",
    "                - Distance Metrics: Use median for Manhattan distance and mean for Euclidean distance.\n",
    "                    -> Requires choosing the distance metric\n",
    "                    \n",
    "                \"\"\"\n",
    "                self.dbscan_centroids[cluster] = np.mean(points, axis=0)\n",
    "\n",
    "        # Build tree with cluster centroids\n",
    "        centroids = [self.dbscan_centroids[c] for c in self.dbscan_centroids if c != -1]\n",
    "        self.valid_clusters = list(self.dbscan_centroids.keys())\n",
    "        if len(centroids) > 0:\n",
    "            centroids = np.array(centroids)\n",
    "            if self.algorithm == \"kd_tree\":\n",
    "                self.tree = KDTree(\n",
    "                    centroids, leaf_size=self.leaf_size, metric=self.tree_metric\n",
    "                )\n",
    "            elif self.algorithm == \"ball_tree\":\n",
    "                self.tree = BallTree(\n",
    "                    centroids, leaf_size=self.leaf_size, metric=self.tree_metric\n",
    "                )\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.ones(len(X))\n",
    "        X = X.values if isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "        if self.tree is None:\n",
    "            return -1 * np.ones(len(X))\n",
    "\n",
    "        # Find nearest centroid\n",
    "        dist, ind = self.tree.query(X, k=1)\n",
    "        nearest_clusters = [self.valid_clusters[i] for i in ind.flatten()]\n",
    "\n",
    "        for i, cluster in enumerate(nearest_clusters):\n",
    "            if cluster in self.svms:\n",
    "                predictions[i] = self.svms[cluster].predict([X[i]])[0]\n",
    "            else:\n",
    "                predictions[i] = -1  # Anomaly if no SVM for cluster\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DB-OC-SVM model with default ocsvm parameters\n",
    "dbocsvm = DBOCSVM_V2(\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"auto\",\n",
    "    nu=0.2,\n",
    "    eps=eps,\n",
    "    min_samples=min_samples,\n",
    "    dbscan_metric=dbscan_tuning_parameters[\"distance_metric\"],\n",
    "    algorithm=\"kd_tree\",  # ball_tree, kd_tree,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting DBSCAN...\n",
      "DBSCAN Fitted...\n",
      "Unique Clusters: [-1  0  1]\n",
      "Cluster Sizes: {-1: 995, 0: 890, 1: 135}\n"
     ]
    }
   ],
   "source": [
    "dbocsvm.fit_cluster(X_encoded, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22543, 125)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>src_bytes</th>\n",
       "      <th>dst_bytes</th>\n",
       "      <th>land</th>\n",
       "      <th>wrong_fragment</th>\n",
       "      <th>urgent</th>\n",
       "      <th>hot</th>\n",
       "      <th>num_failed_logins</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>num_compromised</th>\n",
       "      <th>...</th>\n",
       "      <th>flag_RSTR</th>\n",
       "      <th>flag_S0</th>\n",
       "      <th>flag_S1</th>\n",
       "      <th>flag_S2</th>\n",
       "      <th>flag_S3</th>\n",
       "      <th>flag_SF</th>\n",
       "      <th>flag_SH</th>\n",
       "      <th>attack_binary</th>\n",
       "      <th>attack_categorical</th>\n",
       "      <th>attack_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>neptune</td>\n",
       "      <td>DoS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  src_bytes  dst_bytes  land  wrong_fragment  urgent  hot  \\\n",
       "0       0.0        0.0        0.0   0.0             0.0     0.0  0.0   \n",
       "\n",
       "   num_failed_logins  logged_in  num_compromised  ...  flag_RSTR  flag_S0  \\\n",
       "0                0.0        0.0              0.0  ...        0.0      0.0   \n",
       "\n",
       "   flag_S1  flag_S2  flag_S3  flag_SF  flag_SH  attack_binary  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0             -1   \n",
       "\n",
       "   attack_categorical  attack_class  \n",
       "0             neptune           DoS  \n",
       "\n",
       "[1 rows x 125 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_set_path)\n",
    "print(test_df.shape)\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22543, 122) (22543,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting into X and y\n",
    "X_test = test_df.drop(\n",
    "    columns=[\"attack_binary\", \"attack_categorical\", \"attack_class\"]\n",
    ").values\n",
    "y_test = test_df[\"attack_binary\"].values\n",
    "y_test_class = test_df[\"attack_class\"]\n",
    "\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reconstruction error inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal test samples: 9711\n",
      "Anomaly test samples: 12832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'normal_loss': 0.000964479972240596,\n",
       " 'anomaly_loss': 0.010680428181224482,\n",
       " 'loss_difference': 0.009715948208983886}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate normal and anomaly samples from test set\n",
    "X_test_normal = X_test[y_test == 1]\n",
    "X_test_anomaly = X_test[y_test == -1]\n",
    "\n",
    "print(f\"Normal test samples: {X_test_normal.shape[0]}\")\n",
    "print(f\"Anomaly test samples: {X_test_anomaly.shape[0]}\")\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_test_normal_tensor = torch.FloatTensor(X_test_normal).to(device)\n",
    "X_test_anomaly_tensor = torch.FloatTensor(X_test_anomaly).to(device)\n",
    "\n",
    "# Create DataLoaders for test data evaluation\n",
    "normal_test_dataset = TensorDataset(X_test_normal_tensor)\n",
    "anomaly_test_dataset = TensorDataset(X_test_anomaly_tensor)\n",
    "normal_test_loader = DataLoader(normal_test_dataset, batch_size=256, shuffle=False)\n",
    "anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "def calculate_reconstruction_error(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            outputs = model(x)\n",
    "            # Calculate MSE for each sample\n",
    "            loss = criterion(outputs, x)\n",
    "            loss = loss.mean(dim=1)\n",
    "            total_loss += torch.sum(loss).item()\n",
    "            total_samples += x.size(0)\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "# Function to evaluate a model's reconstruction performance\n",
    "def evaluate_model(model):\n",
    "    normal_loss = calculate_reconstruction_error(model, normal_test_loader)\n",
    "    anomaly_loss = calculate_reconstruction_error(model, anomaly_test_loader)\n",
    "    loss_difference = anomaly_loss - normal_loss\n",
    "\n",
    "    return {\n",
    "        \"normal_loss\": normal_loss,\n",
    "        \"anomaly_loss\": anomaly_loss,\n",
    "        \"loss_difference\": loss_difference,\n",
    "    }\n",
    "\n",
    "\n",
    "reconstruction_error = evaluate_model(autoencoder)\n",
    "reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best parameters and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best autoencoder model:\n",
      "{'input_dim': 122,\n",
      " 'hidden_dims': [96, 64],\n",
      " 'latent_dim': 55,\n",
      " 'activation_type': 'ELU',\n",
      " 'negative_slope': 0.02,\n",
      " 'dropout_rate': 0.1,\n",
      " 'output_activation_type': 'Sigmoid',\n",
      " 'val_loss': 0.00011326836558407006}\n",
      "\n",
      "Reconstruction error:\n",
      "{'normal_loss': 0.000964479972240596,\n",
      " 'anomaly_loss': 0.010680428181224482,\n",
      " 'loss_difference': 0.009715948208983886}\n",
      "\n",
      "Best dbscan parameters\n",
      "{'eps': 7.745111403504765,\n",
      " 'min_samples': 40,\n",
      " 'distance_metric': 'manhattan',\n",
      " 'evaluation_metric': 'silhouette',\n",
      " 'score': 0.6009919047355652,\n",
      " 'n_clusters': 2,\n",
      " 'cluster_data_points': {0: 890, 1: 135, -1: 995}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "autoencoder_architecture = {\n",
    "    \"input_dim\": existing_model_architecture[\"input_dim\"],\n",
    "    \"hidden_dims\": existing_model_architecture[\"hidden_dims\"],\n",
    "    \"latent_dim\": existing_model_architecture[\"latent_dim\"],\n",
    "    \"activation_type\": existing_model_architecture[\"activation_type\"],\n",
    "    \"negative_slope\": existing_model_architecture[\"negative_slope\"],\n",
    "    \"dropout_rate\": existing_model_architecture[\"dropout_rate\"],\n",
    "    \"output_activation_type\": existing_model_architecture[\"output_activation_type\"],\n",
    "    \"val_loss\": checkpoint[\"val_loss\"],\n",
    "}\n",
    "\n",
    "print(\"Best autoencoder model:\")\n",
    "pprint.pprint(autoencoder_architecture, sort_dicts=False)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reconstruction error:\")\n",
    "pprint.pprint(reconstruction_error, sort_dicts=False)\n",
    "print(\"\")\n",
    "\n",
    "best_dbscan_parameters = {\n",
    "    \"eps\": eps,\n",
    "    \"min_samples\": min_samples,\n",
    "    \"distance_metric\": dbscan_tuning_parameters[\"distance_metric\"],\n",
    "    \"evaluation_metric\": dbscan_tuning_parameters[\"evaluation_metric\"],\n",
    "    \"score\": best_trial_dbscan.value,\n",
    "    \"n_clusters\": n_clusters,\n",
    "    \"cluster_data_points\": cluster_data_points,\n",
    "}\n",
    "\n",
    "print(\"Best dbscan parameters\")\n",
    "pprint.pprint(best_dbscan_parameters, sort_dicts=False)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tuning_result = {\n",
    "    \"autoencoder\": autoencoder_architecture,\n",
    "    \"reconstruction_error\": reconstruction_error,\n",
    "    \"dbscan\": best_dbscan_parameters,\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"max_score\": 0,\n",
    "    \"tuning_results\": {},\n",
    "}\n",
    "\n",
    "os.makedirs(\"tuning_results\", exist_ok=True)\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, \"r\") as file:\n",
    "        existing_results = json.load(file)\n",
    "        if existing_results[\"max_score\"] < best_trial_dbscan.value:\n",
    "            with open(results_path, \"w\") as f:\n",
    "                existing_results[\"max_score\"] = best_dbscan_parameters[\"score\"]\n",
    "                tuning_result_id = len(existing_results[\"tuning_results\"])\n",
    "                existing_results[\"tuning_results\"][tuning_result_id] = tuning_result\n",
    "                json.dump(existing_results, f)\n",
    "else:\n",
    "    with open(results_path, \"w\") as f:\n",
    "        results[\"max_score\"] = best_dbscan_parameters[\"score\"]\n",
    "        results[\"tuning_results\"][0] = tuning_result\n",
    "        json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
