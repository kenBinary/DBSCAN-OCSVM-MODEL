{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "dbscan_optuna_storage_path = \"sqlite:///optuna_storage/dbscan_study.db\"\n",
    "ocsvm_optuna_storage_path = \"sqlite:///optuna_storage/dbocsvm_study.db\"\n",
    "\n",
    "train_set_path = \"../../../datasets/train_set.csv\"\n",
    "test_set_path = \"../../../datasets/test_set.csv\"\n",
    "\n",
    "results_path = \"tuning_results/results_dbocsvm_config.json\"\n",
    "\n",
    "test_run = True\n",
    "\n",
    "if test_run:\n",
    "    with_storage_dbscan = False\n",
    "    with_storage_dbocsvm = False\n",
    "    sample_size = 0.01\n",
    "    use_sample = True\n",
    "    ocsvm_trials = 10\n",
    "else:\n",
    "    os.makedirs(\"optuna_storage\", exist_ok=True)\n",
    "    with_storage_dbscan = True\n",
    "    with_storage_dbocsvm = True\n",
    "    sample_size = 0.3\n",
    "    use_sample = True\n",
    "    ocsvm_trials = 100\n",
    "\n",
    "\n",
    "dbscan_tuning_parameters = {\n",
    "    \"evaluation_metric\": \"silhouette\",  # silhouette, calinski_harabasz, davies_bouldin\n",
    "    \"distance_metric\": \"manhattan\",  # manhattan, euclidean, cosine\n",
    "    \"trials\": 0,\n",
    "}\n",
    "dbocsvm_tree_algorithm = \"ball_tree\"  # \"ball_tree\" or \"kd_tree\"\n",
    "\n",
    "existing_model_path = \"../../../autoencoder/autoencoder.pth\"\n",
    "\n",
    "existing_model_architecture = {\n",
    "    \"input_dim\": 15,\n",
    "    \"hidden_dims\": [13, 11],\n",
    "    \"latent_dim\": 9,\n",
    "    \"activation_type\": \"ReLU\",\n",
    "    \"negative_slope\": 1,\n",
    "    \"output_activation_type\": \"Sigmoid\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE\n",
    "override_dbscan_tuning = True\n",
    "dbscan_override_params = {\n",
    "    \"eps\": 0.10361559446127108,\n",
    "    \"min_samples\": 12,\n",
    "    \"distance_metric\": \"manhattan\",\n",
    "    \"score\": 0.9616295695304871,\n",
    "    \"n_clusters\": 29,\n",
    "    \"cluster_data_points\": {\n",
    "        \"0\": 15666,\n",
    "        \"1\": 11398,\n",
    "        \"2\": 10639,\n",
    "        \"3\": 6686,\n",
    "        \"4\": 858,\n",
    "        \"5\": 3568,\n",
    "        \"6\": 51,\n",
    "        \"7\": 2269,\n",
    "        \"8\": 85,\n",
    "        \"9\": 478,\n",
    "        \"10\": 87,\n",
    "        \"11\": 85,\n",
    "        \"12\": 693,\n",
    "        \"13\": 158,\n",
    "        \"14\": 129,\n",
    "        \"15\": 158,\n",
    "        \"16\": 34,\n",
    "        \"17\": 24,\n",
    "        \"18\": 201,\n",
    "        \"19\": 28,\n",
    "        \"20\": 161,\n",
    "        \"21\": 17,\n",
    "        \"22\": 79,\n",
    "        \"23\": 31,\n",
    "        \"24\": 31,\n",
    "        \"25\": 34,\n",
    "        \"26\": 31,\n",
    "        \"27\": 14,\n",
    "        \"28\": 24,\n",
    "        \"-1\": 283,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(train_set_path)\n",
    "\n",
    "if use_sample:\n",
    "    train_df = train_df.sample(frac=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"attack_binary\", \"attack_categorical\"]).values\n",
    "y_train = train_df[\"attack_binary\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"Input dimension: {input_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use existing autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from autoencoder import BatchNormAutoencoder\n",
    "\n",
    "autoencoder = BatchNormAutoencoder(\n",
    "    input_dim=existing_model_architecture[\"input_dim\"],\n",
    "    hidden_dims=existing_model_architecture[\"hidden_dims\"],\n",
    "    latent_dim=existing_model_architecture[\"latent_dim\"],\n",
    "    activation_type=existing_model_architecture[\"activation_type\"],\n",
    "    negative_slope=existing_model_architecture[\"negative_slope\"],\n",
    "    output_activation_type=existing_model_architecture[\"output_activation_type\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(existing_model_path)\n",
    "autoencoder.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbscan tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# extract encoded features\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "X_train_dataset = TensorDataset(X_train_tensor)\n",
    "X_train_loader = DataLoader(X_train_dataset, batch_size=256)\n",
    "\n",
    "\n",
    "X_encoded = []\n",
    "with torch.no_grad():\n",
    "    for data in X_train_loader:\n",
    "        data_x = data[0].to(device)\n",
    "        encoded = autoencoder.encode(data_x)\n",
    "        X_encoded.append(encoded.cpu().numpy())\n",
    "X_encoded = np.vstack(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_final import find_eps_range_with_elbow_method\n",
    "\n",
    "input_dim_encoded = X_encoded.shape[1]\n",
    "\n",
    "k_for_elbow = int((20 + input_dim_encoded * 2) / 2)\n",
    "# CHANGE\n",
    "if not override_dbscan_tuning:\n",
    "    min_eps, max_eps = find_eps_range_with_elbow_method(\n",
    "        X_encoded,\n",
    "        k=k_for_elbow,\n",
    "        plot=False,\n",
    "    )\n",
    "    min_eps, max_eps\n",
    "    print(min_eps, max_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_final import objective_dbscan\n",
    "import optuna\n",
    "\n",
    "dbscan_objective_lambda = lambda trial: objective_dbscan(\n",
    "    trial,\n",
    "    X_encoded=X_encoded,\n",
    "    evaluation_metric=dbscan_tuning_parameters[\"evaluation_metric\"],\n",
    "    eps_range=(min_eps, max_eps),\n",
    "    min_samples_range=(1, input_dim_encoded * 2),\n",
    "    distance_metric=dbscan_tuning_parameters[\"distance_metric\"],\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "if not override_dbscan_tuning:\n",
    "    print(\"Starting DBSCAN tuning...\")\n",
    "    if with_storage_dbscan:\n",
    "        dbscan_study = optuna.create_study(\n",
    "            direction=\"maximize\",\n",
    "            storage=dbscan_optuna_storage_path,\n",
    "            study_name=\"dbscan_study\",\n",
    "            load_if_exists=True,\n",
    "        )\n",
    "        dbscan_study.optimize(\n",
    "            dbscan_objective_lambda,\n",
    "            n_trials=dbscan_tuning_parameters[\"trials\"],\n",
    "        )\n",
    "    else:\n",
    "        dbscan_study = optuna.create_study(direction=\"maximize\")\n",
    "        dbscan_study.optimize(\n",
    "            dbscan_objective_lambda,\n",
    "            n_trials=dbscan_tuning_parameters[\"trials\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from plotly.io import show\n",
    "\n",
    "if not override_dbscan_tuning:\n",
    "    fig = optuna.visualization.plot_optimization_history(dbscan_study)\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not override_dbscan_tuning:\n",
    "    fig = optuna.visualization.plot_edf([dbscan_study])\n",
    "\n",
    "    show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "if override_dbscan_tuning:\n",
    "    eps = dbscan_override_params[\"eps\"]\n",
    "    min_samples = dbscan_override_params[\"min_samples\"]\n",
    "else:\n",
    "    eps = dbscan_study.best_params[\"eps\"]\n",
    "    min_samples = dbscan_study.best_params[\"min_samples\"]\n",
    "\n",
    "if override_dbscan_tuning:\n",
    "    n_clusters = dbscan_override_params[\"n_clusters\"]\n",
    "    cluster_data_points = dbscan_override_params[\"cluster_data_points\"]\n",
    "else:\n",
    "    best_trial_dbscan = dbscan_study.best_trial\n",
    "    best_trial_dbscan_user_attrs = best_trial_dbscan.user_attrs\n",
    "\n",
    "    n_clusters = best_trial_dbscan_user_attrs[\"n_clusters\"]\n",
    "    cluster_data_points = best_trial_dbscan_user_attrs[\"cluster_data_points\"]\n",
    "\n",
    "\n",
    "print(f\"eps = {eps}\")\n",
    "print(f\"min_samples = {min_samples}\")\n",
    "print(f\"n_clusters = {n_clusters}\")\n",
    "print(\"cluster_data_points\")\n",
    "pprint.pprint(cluster_data_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit the DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_ocsvm import DBOCSVM\n",
    "\n",
    "# Create DB-OC-SVM model with default ocsvm parameters\n",
    "\n",
    "if override_dbscan_tuning:\n",
    "    dbscan_distance_metric = dbscan_override_params[\"distance_metric\"]\n",
    "else:\n",
    "    dbscan_distance_metric = dbscan_tuning_parameters[\"distance_metric\"]\n",
    "\n",
    "dbocsvm = DBOCSVM(\n",
    "    kernel=\"rbf\",\n",
    "    gamma=\"auto\",\n",
    "    nu=0.2,\n",
    "    eps=eps,\n",
    "    min_samples=min_samples,\n",
    "    dbscan_distance_metric=dbscan_distance_metric,\n",
    "    tree_algorithm=dbocsvm_tree_algorithm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbocsvm.fit_cluster(X_encoded, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.read_csv(test_set_path)\n",
    "\n",
    "if test_run:\n",
    "    test_dataset = test_dataset.sample(\n",
    "        frac=sample_size * 2, random_state=42\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"test set count: {test_dataset.shape[0]:,}\")\n",
    "\n",
    "\n",
    "print(f\"unique values: {test_dataset['attack_categorical'].value_counts()}\")\n",
    "\n",
    "\n",
    "test_dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into test and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_df, val_df = train_test_split(\n",
    "    test_dataset,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=test_dataset[\"attack_categorical\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into X and y\n",
    "X_test = test_df.drop(columns=[\"attack_binary\", \"attack_categorical\"]).values\n",
    "y_test = test_df[\"attack_binary\"].values\n",
    "y_test_class = test_df[\"attack_categorical\"].values\n",
    "\n",
    "print(f\"test set count: {test_df.shape[0]:,}\")\n",
    "print(f\"unique values: {test_df['attack_categorical'].value_counts()}\")\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into X and y\n",
    "X_val = val_df.drop(columns=[\"attack_binary\", \"attack_categorical\"]).values\n",
    "y_val = val_df[\"attack_binary\"].values\n",
    "y_val_class = val_df[\"attack_categorical\"].values\n",
    "\n",
    "print(f\"val set count: {val_df.shape[0]:,}\")\n",
    "print(f\"unique values: {val_df['attack_categorical'].value_counts()}\")\n",
    "val_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# First, display the original distribution\n",
    "print(\"Before SMOTE:\")\n",
    "print(f\"Val set count: {X_val.shape[0]:,}\")\n",
    "before_counts = pd.Series(y_val_class).value_counts()\n",
    "print(before_counts)\n",
    "\n",
    "# Apply SMOTE to training data using class labels\n",
    "if test_run:\n",
    "    sampling_strategy = {\n",
    "        \"dos\": 100,\n",
    "        \"portScan\": 100,\n",
    "        \"bruteForce\": 100,\n",
    "        \"pingScan\": 100,\n",
    "    }\n",
    "else:\n",
    "    sampling_strategy = {\n",
    "        \"dos\": 4000,\n",
    "        \"portScan\": 4000,\n",
    "        \"bruteForce\": 4000,\n",
    "        \"pingScan\": 4000,\n",
    "    }\n",
    "\n",
    "smote = SMOTE(random_state=42, k_neighbors=3, sampling_strategy=sampling_strategy)\n",
    "X_val_resampled, y_val_resampled = smote.fit_resample(X_val, y_val_class)\n",
    "\n",
    "# Display the distribution after SMOTE\n",
    "print(\"\\nAfter SMOTE:\")\n",
    "print(f\"Val set count: {X_val_resampled.shape[0]:,}\")\n",
    "after_counts = pd.Series(y_val_resampled).value_counts()\n",
    "print(after_counts)\n",
    "\n",
    "# If you need binary labels for further processing, convert back\n",
    "y_val_resampled = np.where(y_val_resampled == \"benign\", 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reconstruction error inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate normal and anomaly samples from test set\n",
    "X_test_normal = X_test[y_test == 1]\n",
    "X_test_anomaly = X_test[y_test == -1]\n",
    "\n",
    "print(f\"Normal test samples: {X_test_normal.shape[0]}\")\n",
    "print(f\"Anomaly test samples: {X_test_anomaly.shape[0]}\")\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_test_normal_tensor = torch.FloatTensor(X_test_normal).to(device)\n",
    "X_test_anomaly_tensor = torch.FloatTensor(X_test_anomaly).to(device)\n",
    "\n",
    "# Create DataLoaders for test data evaluation\n",
    "normal_test_dataset = TensorDataset(X_test_normal_tensor)\n",
    "anomaly_test_dataset = TensorDataset(X_test_anomaly_tensor)\n",
    "normal_test_loader = DataLoader(normal_test_dataset, batch_size=256, shuffle=False)\n",
    "anomaly_test_loader = DataLoader(anomaly_test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "\n",
    "def calculate_reconstruction_error(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0]\n",
    "            outputs = model(x)\n",
    "            # Calculate MSE for each sample\n",
    "            loss = criterion(outputs, x)\n",
    "            loss = loss.mean(dim=1)\n",
    "            total_loss += torch.sum(loss).item()\n",
    "            total_samples += x.size(0)\n",
    "\n",
    "    return total_loss / total_samples\n",
    "\n",
    "\n",
    "# Function to evaluate a model's reconstruction performance\n",
    "def evaluate_model(model):\n",
    "    normal_loss = calculate_reconstruction_error(model, normal_test_loader)\n",
    "    anomaly_loss = calculate_reconstruction_error(model, anomaly_test_loader)\n",
    "    loss_difference = anomaly_loss - normal_loss\n",
    "\n",
    "    return {\n",
    "        \"normal_loss\": normal_loss,\n",
    "        \"anomaly_loss\": anomaly_loss,\n",
    "        \"loss_difference\": loss_difference,\n",
    "    }\n",
    "\n",
    "\n",
    "reconstruction_error = evaluate_model(autoencoder)\n",
    "reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract features from validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tensor = torch.FloatTensor(X_val_resampled).to(device)\n",
    "\n",
    "X_val_dataset_tensor = TensorDataset(X_val_tensor, torch.zeros(len(X_val_tensor)))\n",
    "X_val_loader = DataLoader(X_val_dataset_tensor, batch_size=128)\n",
    "\n",
    "X_val_encoded = []\n",
    "with torch.no_grad():\n",
    "    for data, _ in X_val_loader:\n",
    "        encoded = autoencoder.encode(data)\n",
    "        X_val_encoded.append(encoded.cpu().numpy())\n",
    "\n",
    "X_val_encoded = np.vstack(X_val_encoded)\n",
    "print(X_val_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract features from test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, torch.zeros(len(X_test_tensor)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "X_test_encoded = []\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        encoded = autoencoder.encode(data)\n",
    "        X_test_encoded.append(encoded.cpu().numpy())\n",
    "\n",
    "X_test_encoded = np.vstack(X_test_encoded)\n",
    "print(X_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning the ocsvms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_final import objective_dbocsvm_fit_ocsvm\n",
    "\n",
    "# Inner Optuna study for DBSCAN\n",
    "dbocsvm_fit_ocsvm_objective_lambda = lambda trial: objective_dbocsvm_fit_ocsvm(\n",
    "    trial,\n",
    "    model=dbocsvm,\n",
    "    X_encoded_train=X_encoded,\n",
    "    X_encoded_validation=X_val_encoded,\n",
    "    y_validation=y_val_resampled,\n",
    "    X_encoded_test=X_test_encoded,\n",
    "    y_test=y_test,\n",
    "    cluster_count=n_clusters,\n",
    "    metric=\"f1\",\n",
    ")\n",
    "\n",
    "if with_storage_dbocsvm:\n",
    "    dbocsvm_study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        storage=ocsvm_optuna_storage_path,\n",
    "        study_name=\"dbocsvm_study\",\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "    dbocsvm_study.optimize(\n",
    "        dbocsvm_fit_ocsvm_objective_lambda,\n",
    "        n_trials=ocsvm_trials,\n",
    "    )\n",
    "else:\n",
    "    dbocsvm_study = optuna.create_study(direction=\"maximize\")\n",
    "    dbocsvm_study.optimize(\n",
    "        dbocsvm_fit_ocsvm_objective_lambda,\n",
    "        n_trials=ocsvm_trials,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from plotly.io import show\n",
    "\n",
    "fig = optuna.visualization.plot_optimization_history(dbocsvm_study)\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_edf([dbocsvm_study])\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_list = {}\n",
    "\n",
    "for key, value in dbocsvm_study.best_params.items():\n",
    "    cluster = key.split(\"_\")[1]\n",
    "    cluster = int(cluster)\n",
    "\n",
    "    parameter_list[cluster] = {\n",
    "        \"kernel\": \"rbf\",\n",
    "        \"gamma\": dbocsvm_study.best_params[f\"gamma_{cluster}\"],\n",
    "        \"nu\": dbocsvm_study.best_params[f\"nu_{cluster}\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best parameters and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_architecture = {\n",
    "    \"input_dim\": existing_model_architecture[\"input_dim\"],\n",
    "    \"hidden_dims\": existing_model_architecture[\"hidden_dims\"],\n",
    "    \"latent_dim\": existing_model_architecture[\"latent_dim\"],\n",
    "    \"activation_type\": existing_model_architecture[\"activation_type\"],\n",
    "    \"negative_slope\": existing_model_architecture[\"negative_slope\"],\n",
    "    \"output_activation_type\": existing_model_architecture[\"output_activation_type\"],\n",
    "    \"val_loss\": checkpoint[\"val_loss\"],\n",
    "}\n",
    "\n",
    "print(\"Best autoencoder model:\")\n",
    "pprint.pprint(autoencoder_architecture, sort_dicts=False)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reconstruction error:\")\n",
    "pprint.pprint(reconstruction_error, sort_dicts=False)\n",
    "print(\"\")\n",
    "\n",
    "best_dbscan_parameters = {\n",
    "    \"eps\": eps,\n",
    "    \"min_samples\": min_samples,\n",
    "    \"distance_metric\": dbscan_tuning_parameters[\"distance_metric\"],\n",
    "    \"evaluation_metric\": dbscan_tuning_parameters[\"evaluation_metric\"],\n",
    "    \"score\": best_trial_dbscan.value if not override_dbscan_tuning else dbscan_override_params[\"score\"],\n",
    "    \"n_clusters\": n_clusters,\n",
    "    \"cluster_data_points\": cluster_data_points,\n",
    "}\n",
    "\n",
    "print(\"Best dbscan parameters\")\n",
    "pprint.pprint(best_dbscan_parameters, sort_dicts=False)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Best ocsvm parameters\")\n",
    "print(f\"Tree algorithm: {dbocsvm_tree_algorithm}\")\n",
    "print(f\"Accuracy: {dbocsvm_study.best_value}\")\n",
    "pprint.pprint(parameter_list, sort_dicts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tuning_result = {\n",
    "    \"dbscan\": best_dbscan_parameters,\n",
    "    \"ocsvm\": {\n",
    "        \"tree_algorithm\": dbocsvm_tree_algorithm,\n",
    "        \"accuracy\": dbocsvm_study.best_value,\n",
    "        \"parameters\": parameter_list,\n",
    "    },\n",
    "}\n",
    "\n",
    "results = {\n",
    "    \"max_score\": 0,\n",
    "    \"autoencoder_architecture\": autoencoder_architecture,\n",
    "    \"reconstruction_error\": reconstruction_error,\n",
    "    \"tuning_results\": {},\n",
    "}\n",
    "\n",
    "os.makedirs(\"tuning_results\", exist_ok=True)\n",
    "if os.path.exists(results_path):\n",
    "    with open(results_path, \"r\") as file:\n",
    "        existing_results = json.load(file)\n",
    "        if existing_results[\"max_score\"] < dbocsvm_study.best_value:\n",
    "            with open(results_path, \"w\") as f:\n",
    "                existing_results[\"max_score\"] = dbocsvm_study.best_value\n",
    "                tuning_result_id = len(existing_results[\"tuning_results\"])\n",
    "                tuning_result[\"score\"] = dbocsvm_study.best_value\n",
    "                existing_results[\"tuning_results\"][tuning_result_id] = tuning_result\n",
    "                json.dump(existing_results, f)\n",
    "else:\n",
    "    with open(results_path, \"w\") as f:\n",
    "        results[\"max_score\"] = dbocsvm_study.best_value\n",
    "        tuning_result[\"score\"] = dbocsvm_study.best_value\n",
    "        results[\"tuning_results\"][0] = tuning_result\n",
    "        json.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
